{"ast":null,"code":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar tfc = require(\"@tensorflow/tfjs-core\");\n\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\n\nvar common_1 = require(\"../common\");\n\nvar errors_1 = require(\"../errors\");\n\nvar types_1 = require(\"../types\");\n\nvar generic_utils_1 = require(\"../utils/generic_utils\");\n\nvar math_utils = require(\"../utils/math_utils\");\n\nvar common_2 = require(\"./common\");\n\nvar common_3 = require(\"./common\");\n\nvar backend = 'webgl';\nvar DEFAULT_DTYPE = types_1.DType.float32;\n\nfunction disposeScalarCache() {\n  for (var typeKey in scalarCache) {\n    for (var key in scalarCache[typeKey]) {\n      scalarCache[typeKey][key].dispose();\n      delete scalarCache[typeKey][key];\n    }\n  }\n}\n\nexports.disposeScalarCache = disposeScalarCache;\n\nfunction setBackend(requestedBackend) {\n  tfc.setBackend(requestedBackend);\n  backend = requestedBackend;\n  disposeScalarCache();\n}\n\nexports.setBackend = setBackend;\n\nfunction getBackend() {\n  return backend;\n}\n\nexports.getBackend = getBackend;\n\nfunction keep(x) {\n  return tfc.keep(x);\n}\n\nexports.keep = keep;\nvar scalarCache = {\n  float32: {},\n  int32: {}\n};\n\nfunction getScalar(value, dtype) {\n  if (dtype === undefined) {\n    dtype = DEFAULT_DTYPE;\n  }\n\n  if (scalarCache[dtype][value] == null) {\n    scalarCache[dtype][value] = tfjs_core_1.scalar(value, dtype);\n    tfc.keep(scalarCache[dtype][value]);\n  }\n\n  return scalarCache[dtype][value];\n}\n\nexports.getScalar = getScalar;\nexports.epsilon = common_2.epsilon;\n\nfunction isBackendSymbolic() {\n  return false;\n}\n\nexports.isBackendSymbolic = isBackendSymbolic;\n\nfunction shape(x) {\n  return x.shape;\n}\n\nexports.shape = shape;\n\nfunction intShape(x) {\n  return x.shape;\n}\n\nexports.intShape = intShape;\n\nfunction ndim(x) {\n  return x.shape.length;\n}\n\nexports.ndim = ndim;\n\nfunction dtype(x) {\n  return x instanceof tfjs_core_1.Tensor ? DEFAULT_DTYPE : x.dtype;\n}\n\nexports.dtype = dtype;\n\nfunction normalizeAxis(x, axis) {\n  if (axis == null) {\n    return axis;\n  }\n\n  var xShape = shape(x);\n\n  if (Array.isArray(axis)) {\n    return axis.map(function (thisAxis) {\n      return generic_utils_1.pyNormalizeArrayIndex(xShape, thisAxis);\n    });\n  }\n\n  return generic_utils_1.pyNormalizeArrayIndex(xShape, axis);\n}\n\nexports.normalizeAxis = normalizeAxis;\n\nfunction countParams(x) {\n  var shape = x.shape;\n\n  if (shape.length > 0) {\n    return shape.reduce(function (a, b) {\n      return a * b;\n    });\n  } else {\n    return 1;\n  }\n}\n\nexports.countParams = countParams;\n\nfunction cast(x, dtype) {\n  return x.asType(dtype);\n}\n\nexports.cast = cast;\n\nfunction reshape(x, shape) {\n  return x.reshape(shape);\n}\n\nexports.reshape = reshape;\n\nfunction transpose(x, perm) {\n  return tfc.transpose(x, perm);\n}\n\nexports.transpose = transpose;\nexports.permuteDimensions = transpose;\n\nfunction reverse(x, axes) {\n  return tfc.reverse(x, axes);\n}\n\nexports.reverse = reverse;\n\nfunction expandDims(x, axis) {\n  if (axis === void 0) {\n    axis = -1;\n  }\n\n  var outShape = shape(x).slice();\n\n  if (axis < 0) {\n    axis = outShape.length + axis + 1;\n  }\n\n  outShape.splice(axis, 0, 1);\n  return reshape(x, outShape);\n}\n\nexports.expandDims = expandDims;\n\nfunction squeeze(x, axis) {\n  return tfc.squeeze(x, [axis]);\n}\n\nexports.squeeze = squeeze;\n\nfunction temporalPadding(x, padding) {\n  if (ndim(x) !== 3) {\n    throw new errors_1.ValueError(\"temporalPadding expects input tensor to be 3-D, but received a \" + (ndim(x) + \"-D tensor.\"));\n  }\n\n  if (padding == null) {\n    padding = [1, 1];\n  }\n\n  if (padding.length !== 2) {\n    throw new errors_1.ValueError(\"temporalPadding expects input padding pattern to be a length-2 \" + (\"array, but received a length-\" + padding.length + \" array.\"));\n  }\n\n  var pattern = [[0, 0], padding, [0, 0]];\n  return tfc.pad(x, pattern);\n}\n\nexports.temporalPadding = temporalPadding;\n\nfunction spatial2dPadding(x, padding, dataFormat) {\n  if (ndim(x) !== 4) {\n    throw new errors_1.ValueError(\"temporalPadding expects input tensor to be 4-D, but received a \" + (ndim(x) + \"-D tensor.\"));\n  }\n\n  if (padding == null) {\n    padding = [[1, 1], [1, 1]];\n  }\n\n  if (padding.length !== 2 || padding[0].length !== 2 || padding[1].length !== 2) {\n    throw new errors_1.ValueError('spatial2dPadding expects `padding` to be an Array of two Arrays, ' + 'each of which is an Array of two integers.');\n  }\n\n  if (dataFormat == null) {\n    dataFormat = common_3.imageDataFormat();\n  }\n\n  if (dataFormat !== 'channelsLast' && dataFormat !== 'channelsFirst') {\n    throw new errors_1.ValueError(\"Unknown data format: \" + dataFormat + \". \" + \"Supported data formats are 'channelsLast' and 'channelsFirst.\");\n  }\n\n  var pattern;\n\n  if (dataFormat === 'channelsFirst') {\n    pattern = [[0, 0], [0, 0], padding[0], padding[1]];\n  } else {\n    pattern = [[0, 0], padding[0], padding[1], [0, 0]];\n  }\n\n  return tfc.pad(x, pattern);\n}\n\nexports.spatial2dPadding = spatial2dPadding;\n\nfunction repeat(x, n) {\n  if (x.shape.length !== 2) {\n    throw new errors_1.ValueError(\"repeat() expects a rank-2 tensor, but received a \" + (\"rank-\" + x.shape.length + \" tensor.\"));\n  }\n\n  var y = expandDims(x, 1);\n  return tile(y, [1, n, 1]);\n}\n\nexports.repeat = repeat;\n\nfunction flatten(x) {\n  var newShape = [math_utils.arrayProd(x.shape)];\n  return reshape(x, newShape);\n}\n\nexports.flatten = flatten;\n\nfunction batchFlatten(x) {\n  if (ndim(x) <= 1) {\n    throw new errors_1.ValueError(\"batchFlatten requires a minimum rank of 2. Got rank: \" + ndim(x) + \".\");\n  }\n\n  var newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n  return reshape(x, newShape);\n}\n\nexports.batchFlatten = batchFlatten;\n\nfunction sliceAlongFirstAxis(array, start, size) {\n  switch (array.rank) {\n    case 1:\n      return tfc.slice1d(array, start, size);\n\n    case 2:\n      return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);\n\n    case 3:\n      return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);\n\n    case 4:\n      return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);\n\n    default:\n      throw new errors_1.ValueError(\"sliceAlongFirstAxis() received an unsupported tensor rank: \" + (\"\" + array.rank));\n  }\n}\n\nexports.sliceAlongFirstAxis = sliceAlongFirstAxis;\n\nfunction sliceAlongLastAxis(array, start, size) {\n  switch (array.rank) {\n    case 1:\n      return tfc.slice1d(array, start, size);\n\n    case 2:\n      return tfc.slice2d(array, [0, start], [array.shape[0], size]);\n\n    case 3:\n      return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);\n\n    case 4:\n      return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);\n\n    default:\n      throw new errors_1.ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" + (\"\" + array.rank));\n  }\n}\n\nexports.sliceAlongLastAxis = sliceAlongLastAxis;\n\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon) {\n  if (epsilon === void 0) {\n    epsilon = 1e-3;\n  }\n\n  return tfjs_core_1.tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon) {\n  if (epsilon === void 0) {\n    epsilon = 1e-3;\n  }\n\n  return tfjs_core_1.tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var targetShape = [];\n\n    for (var _i = 0, _a = math_utils.range(0, ndim(x)); _i < _a.length; _i++) {\n      var axis = _a[_i];\n\n      if (reductionAxes.indexOf(axis) !== -1) {\n        targetShape.push(1);\n      } else {\n        targetShape.push(x.shape[axis]);\n      }\n    }\n\n    var broadcastMean = reshape(mean, targetShape);\n    var broadcastVariance = reshape(variance, targetShape);\n    var broadcastGamma = gamma == null ? null : reshape(gamma, targetShape);\n    var broadcastBeta = beta == null ? null : reshape(beta, targetShape);\n    var normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n\nfunction normalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon) {\n  if (epsilon === void 0) {\n    epsilon = 1e-3;\n  }\n\n  if (tfjs_core_1.util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, ndim(x) - 1))) {\n    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  }\n}\n\nexports.normalizeBatchInTraining = normalizeBatchInTraining;\n\nfunction concatenate(tensors, axis) {\n  if (axis === void 0) {\n    axis = -1;\n  }\n\n  var rank;\n\n  if (axis < 0) {\n    rank = ndim(tensors[0]);\n\n    if (rank !== 0) {\n      axis = rank;\n    } else {\n      axis = 0;\n    }\n  }\n\n  if (axis === ndim(tensors[0])) {\n    axis = -1;\n  }\n\n  return tfc.concat(tensors, axis);\n}\n\nexports.concatenate = concatenate;\n\nfunction concatAlongFirstAxis(a, b) {\n  switch (a.rank) {\n    case 1:\n      return tfc.concat1d([a, b]);\n\n    case 2:\n      return tfc.concat2d([a, b], 0);\n\n    case 3:\n      return tfc.concat3d([a, b], 0);\n\n    case 4:\n      return tfc.concat4d([a, b], 0);\n\n    default:\n      throw new errors_1.ValueError('concatAlongFirstAxis() received an unsupported tensor rank: ' + a.rank);\n  }\n}\n\nexports.concatAlongFirstAxis = concatAlongFirstAxis;\n\nfunction tile(x, n) {\n  if (!Array.isArray(n)) {\n    n = [n];\n  }\n\n  if (ndim(x) !== n.length) {\n    throw new errors_1.ValueError(\"The length of input n (\" + n.length + \") does not match \" + (\"the number of dimensions in input x (\" + ndim(x) + \")\"));\n  }\n\n  return tfc.tile(x, n);\n}\n\nexports.tile = tile;\n\nfunction variable(x, dtype, name, constraint) {\n  return new types_1.LayerVariable(x, dtype, name, true, constraint);\n}\n\nexports.variable = variable;\n\nfunction batchGetValue(xs) {\n  return xs.map(function (x) {\n    return x.read();\n  });\n}\n\nexports.batchGetValue = batchGetValue;\n\nfunction batchSetValue(variablesAndValues) {\n  variablesAndValues.map(function (variableAndValue) {\n    var variable = variableAndValue[0];\n    variable.write(variableAndValue[1]);\n  });\n}\n\nexports.batchSetValue = batchSetValue;\n\nfunction zeros(shape, dtype) {\n  return tfc.zeros(shape);\n}\n\nexports.zeros = zeros;\n\nfunction zerosVariable(shape, dtype, name) {\n  return new types_1.LayerVariable(zeros(shape), dtype, name);\n}\n\nexports.zerosVariable = zerosVariable;\n\nfunction zerosLike(x, dtype, name) {\n  return new types_1.LayerVariable(tfc.zerosLike(x), dtype, name);\n}\n\nexports.zerosLike = zerosLike;\n\nfunction ones(shape, dtype) {\n  return tfc.ones(shape);\n}\n\nexports.ones = ones;\n\nfunction onesVariable(shape, dtype, name) {\n  var allocated = tfc.ones(shape);\n  return new types_1.LayerVariable(allocated, dtype, name);\n}\n\nexports.onesVariable = onesVariable;\n\nfunction onesLike(x, dtype, name) {\n  var allocated = tfc.onesLike(x);\n  return new types_1.LayerVariable(allocated, dtype, name);\n}\n\nexports.onesLike = onesLike;\n\nfunction identity(x) {\n  return x.clone();\n}\n\nexports.identity = identity;\n\nfunction eye(size, dtype, name) {\n  var buffer = [];\n\n  for (var i = 0; i < size; ++i) {\n    for (var j = 0; j < size; ++j) {\n      buffer.push(i === j ? 1 : 0);\n    }\n  }\n\n  return tfjs_core_1.tensor2d(buffer, [size, size]);\n}\n\nexports.eye = eye;\n\nfunction eyeVariable(size, dtype, name) {\n  return new types_1.LayerVariable(eye(size, dtype), dtype, name);\n}\n\nexports.eyeVariable = eyeVariable;\n\nfunction neg(x) {\n  return tfc.neg(x);\n}\n\nexports.neg = neg;\n\nfunction add(x, y) {\n  return tfc.add(x, y);\n}\n\nexports.add = add;\n\nfunction subtract(x, y) {\n  return tfc.sub(x, y);\n}\n\nexports.subtract = subtract;\n\nfunction multiply(x, y) {\n  return tfc.mul(x, y);\n}\n\nexports.multiply = multiply;\n\nfunction divide(x, y) {\n  return tfc.div(x, y);\n}\n\nexports.divide = divide;\n\nfunction scalarTimesArray(c, x) {\n  return tfc.mul(c, x);\n}\n\nexports.scalarTimesArray = scalarTimesArray;\n\nfunction scalarPlusArray(c, x) {\n  return tfc.add(c, x);\n}\n\nexports.scalarPlusArray = scalarPlusArray;\n\nfunction randomUniform(shape, minval, maxval, dtype, seed) {\n  return tfc.randomUniform(shape, minval, maxval);\n}\n\nexports.randomUniform = randomUniform;\n\nfunction randomUniformVariable(shape, minval, maxval, dtype, seed, name) {\n  if (name === void 0) {\n    name = 'randomUniform';\n  }\n\n  return new types_1.LayerVariable(randomUniform(shape, minval, maxval, dtype, seed), dtype, name);\n}\n\nexports.randomUniformVariable = randomUniformVariable;\n\nfunction truncatedNormal(shape, mean, stddev, dtype, seed) {\n  if (mean === void 0) {\n    mean = 0.0;\n  }\n\n  if (stddev === void 0) {\n    stddev = 1.0;\n  }\n\n  return tfc.truncatedNormal(shape, mean, stddev);\n}\n\nexports.truncatedNormal = truncatedNormal;\n\nfunction truncatedNormalVariable(shape, mean, stddev, dtype, seed, name) {\n  if (mean === void 0) {\n    mean = 0.0;\n  }\n\n  if (stddev === void 0) {\n    stddev = 1.0;\n  }\n\n  if (name === void 0) {\n    name = 'truncatedNormal';\n  }\n\n  return new types_1.LayerVariable(truncatedNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n\nexports.truncatedNormalVariable = truncatedNormalVariable;\n\nfunction randomNormal(shape, mean, stddev, dtype, seed) {\n  if (mean === void 0) {\n    mean = 0.0;\n  }\n\n  if (stddev === void 0) {\n    stddev = 1.0;\n  }\n\n  if (dtype === types_1.DType.bool) {\n    throw new errors_1.NotImplementedError(\"randomNormal does not support dType bool.\");\n  }\n\n  var dtypeString = dtype === types_1.DType.float32 ? 'float32' : 'int32';\n  return tfc.randomNormal(shape, mean, stddev, dtypeString, seed);\n}\n\nexports.randomNormal = randomNormal;\n\nfunction randomNormalVariable(shape, mean, stddev, dtype, seed, name) {\n  if (mean === void 0) {\n    mean = 0.0;\n  }\n\n  if (stddev === void 0) {\n    stddev = 1.0;\n  }\n\n  if (name === void 0) {\n    name = 'randomNormal';\n  }\n\n  return new types_1.LayerVariable(randomNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n\nexports.randomNormalVariable = randomNormalVariable;\n\nfunction update(x, xNew) {\n  return x.write(xNew);\n}\n\nexports.update = update;\n\nfunction updateAdd(x, increment) {\n  return x.write(tfc.add(x.read(), increment));\n}\n\nexports.updateAdd = updateAdd;\n\nfunction updateSub(x, decrement) {\n  return x.write(tfc.sub(x.read(), decrement));\n}\n\nexports.updateSub = updateSub;\n\nfunction dot(x, y) {\n  if (ndim(y) !== 2) {\n    throw new errors_1.NotImplementedError(\"dot support for y other than rank 2 is not yet implemented: \" + (\"y shape = \" + shape));\n  } else {\n    if (ndim(x) === 2) {\n      return tfc.matMul(x, y);\n    } else if (ndim(x) === 3) {\n      var xShape0 = x.shape[0];\n      var xShape1 = x.shape[1];\n      var xShape2 = x.shape[2];\n      x = x.reshape([xShape0 * xShape1, xShape2]);\n      return tfc.matMul(x, y).reshape([xShape0, xShape1, y.shape[1]]);\n    } else {\n      throw new errors_1.NotImplementedError(\"dot support for x of rank \" + ndim(x) + \" is not yet implemented: \" + (\"x shape = \" + shape));\n    }\n  }\n}\n\nexports.dot = dot;\n\nfunction sign(x) {\n  var zerosLikeX = tfjs_core_1.zerosLike(x);\n  var onesLikeX = tfjs_core_1.onesLike(x);\n  return tfjs_core_1.where(equal(x, zerosLikeX), zerosLikeX, tfjs_core_1.where(greater(x, tfjs_core_1.zerosLike(x)), onesLikeX, scalarTimesArray(getScalar(-1), onesLikeX)));\n}\n\nexports.sign = sign;\n\nfunction qr(x) {\n  if (x.shape.length !== 2) {\n    throw new errors_1.ValueError(\"qr() requires a 2D Tensor, but got a \" + x.shape.length + \"D Tensor.\");\n  }\n\n  if (x.shape[0] < x.shape[1]) {\n    throw new errors_1.ValueError(\"qr() requires x.shape[0] >= x.shape[1], but got shape: [\" + x.shape + \"]\");\n  }\n\n  var m = x.shape[0];\n  var n = x.shape[1];\n  var q = eye(m);\n  var r = x;\n  var one2D = tfjs_core_1.tensor2d([[1]], [1, 1]);\n\n  for (var j = 0; j < n; ++j) {\n    var rjEnd1 = r.slice([j, j], [m - j, 1]);\n    var normX = tfc.norm(rjEnd1);\n    var rjj = r.slice([j, j], [1, 1]);\n    var s = tfc.neg(sign(rjj));\n    var u1 = rjj.sub(multiply(s, normX));\n    var wPre = divide(rjEnd1, u1);\n    var w = void 0;\n\n    if (wPre.shape[0] === 1) {\n      w = one2D;\n    } else {\n      w = one2D.concat(wPre.slice([1, 0], [wPre.shape[0] - 1, wPre.shape[1]]), 0);\n    }\n\n    var tau = tfc.neg(divide(tfc.matMul(s, u1), normX));\n    var rjEndAll = r.slice([j, 0], [m - j, n]);\n    var tauTimesW = tau.mul(w);\n\n    if (j === 0) {\n      r = rjEndAll.sub(tauTimesW.matMul(w.transpose().matMul(rjEndAll)));\n    } else {\n      r = r.slice([0, 0], [j, n]).concat(rjEndAll.sub(tauTimesW.matMul(w.transpose().matMul(rjEndAll))), 0);\n    }\n\n    var qAllJEnd = q.slice([0, j], [m, q.shape[1] - j]);\n\n    if (j === 0) {\n      q = qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tauTimesW.transpose()));\n    } else {\n      q = q.slice([0, 0], [m, j]).concat(qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tauTimesW.transpose())), 1);\n    }\n  }\n\n  return [q, r];\n}\n\nexports.qr = qr;\n\nfunction oneHot(indices, numClasses) {\n  if (ndim(indices) !== 1) {\n    throw new Error('Only 1D one-hot tensors are supported in the ' + 'deeplearn backend, at present.');\n  }\n\n  indices = indices.toInt();\n  return tfc.oneHot(indices, numClasses).toFloat();\n}\n\nexports.oneHot = oneHot;\n\nfunction mean(x, axis, keepDims) {\n  axis = normalizeAxis(x, axis);\n  return tfc.mean(x, axis, keepDims);\n}\n\nexports.mean = mean;\n\nfunction argmax(x, axis) {\n  if (axis === void 0) {\n    axis = -1;\n  }\n\n  return tfc.argMax(x, axis);\n}\n\nexports.argmax = argmax;\n\nfunction gather(reference, indices, axis) {\n  if (Array.isArray(indices)) {\n    indices = tfjs_core_1.tensor1d(indices, 'int32');\n  } else {\n    indices = indices.toInt();\n  }\n\n  return tfc.gather(reference, indices, axis);\n}\n\nexports.gather = gather;\n\nfunction max(x, axis, keepDims) {\n  return tfc.max(x, axis, keepDims);\n}\n\nexports.max = max;\n\nfunction min(x, axis, keepDims) {\n  return tfc.min(x, axis, keepDims);\n}\n\nexports.min = min;\n\nfunction minimum(x, y) {\n  return tfc.minimum(x, y);\n}\n\nexports.minimum = minimum;\n\nfunction sum(x, axis, keepDims) {\n  return tfc.sum(x, axis, keepDims);\n}\n\nexports.sum = sum;\n\nfunction abs(x) {\n  return tfc.abs(x);\n}\n\nexports.abs = abs;\n\nfunction square(x) {\n  return tfc.mulStrict(x, x);\n}\n\nexports.square = square;\n\nfunction sqrt(x) {\n  return tfc.sqrt(x);\n}\n\nexports.sqrt = sqrt;\n\nfunction exp(x) {\n  return tfc.exp(x);\n}\n\nexports.exp = exp;\n\nfunction log(x) {\n  return tfc.log(x);\n}\n\nexports.log = log;\n\nfunction pow(x, a) {\n  if (typeof a === 'number') {\n    a = tfjs_core_1.scalar(Math.round(a), 'int32');\n  }\n\n  if (a.dtype !== 'int32') {\n    throw new errors_1.NotImplementedError(\"Non-int32 dtype (\" + a.dtype + \") is not supported by pow() yet\");\n  }\n\n  return tfc.pow(x, a);\n}\n\nexports.pow = pow;\n\nfunction clip(x, minValue, maxValue) {\n  return tfc.clipByValue(x, minValue, maxValue);\n}\n\nexports.clip = clip;\n\nfunction equal(x, y) {\n  return tfc.equal(x, y);\n}\n\nexports.equal = equal;\n\nfunction greater(x, y) {\n  return tfc.greater(x, y);\n}\n\nexports.greater = greater;\n\nfunction greaterEqual(x, y) {\n  return tfc.greaterEqual(x, y);\n}\n\nexports.greaterEqual = greaterEqual;\n\nfunction maximum(x, y) {\n  return tfc.maximum(x, y);\n}\n\nexports.maximum = maximum;\n\nfunction sin(x) {\n  return tfc.sin(x.value());\n}\n\nexports.sin = sin;\n\nfunction cos(x) {\n  return tfc.cos(x.value());\n}\n\nexports.cos = cos;\n\nfunction batchNormalization(x, mean, variance, beta, gamma, epsilon) {\n  if (epsilon === void 0) {\n    epsilon = 1e-3;\n  }\n\n  var out;\n\n  if (ndim(x) === 2) {\n    out = tfc.batchNormalization2d(x, mean, variance, epsilon, gamma, beta);\n  } else if (ndim(x) === 3) {\n    out = tfc.batchNormalization3d(x, mean, variance, epsilon, gamma, beta);\n  } else if (ndim(x) === 4) {\n    out = tfc.batchNormalization4d(x, mean, variance, epsilon, gamma, beta);\n  } else {\n    throw new errors_1.NotImplementedError(\"batchNormalization is not implememnted for array of rank \" + ndim(x) + \" \" + \"yet\");\n  }\n\n  return out;\n}\n\nexports.batchNormalization = batchNormalization;\n\nfunction biasAdd(x, bias, dataFormat) {\n  if (dataFormat == null) {\n    dataFormat = common_3.imageDataFormat();\n  }\n\n  common_1.checkDataFormat(dataFormat);\n\n  if (ndim(bias) !== 1 && ndim(bias) !== ndim(x)) {\n    throw new errors_1.ValueError('Unexpected bias dimensions: ' + ndim(bias) + '; expected it to be 1 or ' + ndim(x));\n  }\n\n  var biasShape = bias.shape;\n  var y;\n\n  if (ndim(x) === 5) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        y = x.add(bias.reshape([1, biasShape[0], 1, 1, 1]));\n      } else {\n        y = x.add(bias.reshape([1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]));\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        y = x.add(bias.reshape([1, 1, 1, 1, biasShape[0]]));\n      } else {\n        y = x.add(bias.reshape([1].concat(biasShape)));\n      }\n    }\n  } else if (ndim(x) === 4) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        y = x.add(bias.reshape([1, biasShape[0], 1, 1]));\n      } else {\n        y = x.add(bias.reshape([1, biasShape[2], biasShape[0], biasShape[1]]));\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        y = x.add(bias.reshape([1, 1, 1, biasShape[0]]));\n      } else {\n        y = x.add(bias.reshape([1].concat(biasShape)));\n      }\n    }\n  } else if (ndim(x) === 3) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        y = x.add(bias.reshape([1, biasShape[0], 1]));\n      } else {\n        y = x.add(bias.reshape([1, biasShape[1], biasShape[0]]));\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        y = x.add(bias.reshape([1, 1, biasShape[0]]));\n      } else {\n        y = x.add(bias.reshape([1].concat(biasShape)));\n      }\n    }\n  } else if (ndim(x) < 3) {\n    y = x.add(bias);\n  } else {\n    throw new errors_1.ValueError(\"Unsupported input rank by biasAdd: \" + ndim(x));\n  }\n\n  return y;\n}\n\nexports.biasAdd = biasAdd;\n\nfunction elu(x, alpha) {\n  if (alpha === void 0) {\n    alpha = 1;\n  }\n\n  if (alpha !== 1) {\n    throw new errors_1.NotImplementedError(\"Support for alpha values other than 1 (\" + alpha + \") is not implemented \" + \"yet.\");\n  }\n\n  return tfc.elu(x);\n}\n\nexports.elu = elu;\n\nfunction selu(x) {\n  return tfc.selu(x);\n}\n\nexports.selu = selu;\n\nfunction relu(x) {\n  return tfc.relu(x);\n}\n\nexports.relu = relu;\n\nfunction softplus(x) {\n  return tfc.log(tfc.add(getScalar(1), tfc.exp(x)));\n}\n\nexports.softplus = softplus;\n\nfunction softsign(x) {\n  return tfc.div(x, tfc.add(getScalar(1), tfc.abs(x)));\n}\n\nexports.softsign = softsign;\n\nfunction tanh(x) {\n  return tfc.tanh(x);\n}\n\nexports.tanh = tanh;\n\nfunction dropout(x, level, noiseShape, seed) {\n  if (noiseShape != null && !tfjs_core_1.util.arraysEqual(x.shape, noiseShape)) {\n    throw new errors_1.NotImplementedError('Non-default noise shape is not implemented yet: ' + JSON.stringify(noiseShape));\n  }\n\n  if (seed != null) {\n    throw new errors_1.NotImplementedError('seed is not implemented for dropout yet.');\n  }\n\n  var multiplier = tfc.step(tfc.add(neg(level), randomUniform(x.shape, 0, 1, types_1.DType.float32)));\n  multiplier = tfc.mul(divide(getScalar(1), subtract(getScalar(1), level)), multiplier);\n  return tfc.mul(x, multiplier);\n}\n\nexports.dropout = dropout;\n\nfunction l2Normalize(x, axis) {\n  var squareSum = sum(square(x), axis, true);\n  var epsilonTensor = scalarTimesArray(tfjs_core_1.scalar(exports.epsilon()), tfc.onesLike(x));\n  var norm = sqrt(maximum(squareSum, epsilonTensor));\n  return divide(x, norm);\n}\n\nexports.l2Normalize = l2Normalize;\n\nfunction preprocessConv2DInput(x, dataFormat) {\n  common_1.checkDataFormat(dataFormat);\n\n  if (dataFormat === 'channelsFirst') {\n    return tfc.transpose(x, [0, 2, 3, 1]);\n  } else {\n    return x;\n  }\n}\n\nfunction conv1dWithBias(x, kernel, bias, strides, padding, dataFormat, dilationRate) {\n  if (strides === void 0) {\n    strides = 1;\n  }\n\n  if (padding === void 0) {\n    padding = 'valid';\n  }\n\n  if (dilationRate === void 0) {\n    dilationRate = 1;\n  }\n\n  if (dataFormat == null) {\n    dataFormat = common_3.imageDataFormat();\n  }\n\n  common_1.checkDataFormat(dataFormat);\n\n  if (x.shape.length !== 3) {\n    throw new errors_1.ValueError(\"The input of a conv1dWithBias operation should be 3, but is \" + (x.shape.length + \" instead.\"));\n  }\n\n  if (kernel.shape.length !== 3) {\n    throw new errors_1.ValueError(\"The kernel for a conv1dWithBias operation should be 3, but is \" + (kernel.shape.length + \" instead\"));\n  }\n\n  if (bias != null && bias.shape.length !== 1) {\n    throw new errors_1.ValueError(\"The bias for a conv1dWithBias operation should be 1, but is \" + (kernel.shape.length + \" instead\"));\n  }\n\n  if (dataFormat === 'channelsFirst') {\n    x = transpose(x, [0, 2, 1]);\n  }\n\n  if (padding === 'casual') {\n    throw new errors_1.NotImplementedError('The support for CASUAL padding mode in conv1dWithBias is not ' + 'implemented yet.');\n  }\n\n  var y = tfc.conv1d(x, kernel, strides, padding === 'same' ? 'same' : 'valid', 'NWC', dilationRate);\n\n  if (bias != null) {\n    y = biasAdd(y, bias);\n  }\n\n  return y;\n}\n\nexports.conv1dWithBias = conv1dWithBias;\n\nfunction conv1d(x, kernel, strides, padding, dataFormat, dilationRate) {\n  if (strides === void 0) {\n    strides = 1;\n  }\n\n  if (padding === void 0) {\n    padding = 'valid';\n  }\n\n  if (dilationRate === void 0) {\n    dilationRate = 1;\n  }\n\n  common_1.checkDataFormat(dataFormat);\n  return conv1dWithBias(x, kernel, null, strides, padding, dataFormat, dilationRate);\n}\n\nexports.conv1d = conv1d;\n\nfunction conv2d(x, kernel, strides, padding, dataFormat, dilationRate) {\n  if (strides === void 0) {\n    strides = [1, 1];\n  }\n\n  if (padding === void 0) {\n    padding = 'valid';\n  }\n\n  common_1.checkDataFormat(dataFormat);\n  return conv2dWithBias(x, kernel, null, strides, padding, dataFormat, dilationRate);\n}\n\nexports.conv2d = conv2d;\n\nfunction conv2dWithBias(x, kernel, bias, strides, padding, dataFormat, dilationRate) {\n  if (strides === void 0) {\n    strides = [1, 1];\n  }\n\n  if (padding === void 0) {\n    padding = 'valid';\n  }\n\n  if (dataFormat == null) {\n    dataFormat = common_3.imageDataFormat();\n  }\n\n  common_1.checkDataFormat(dataFormat);\n\n  if (ndim(x) !== 3 && ndim(x) !== 4) {\n    throw new errors_1.ValueError(\"conv2dWithBias expects input to be of rank 3 or 4, but received \" + (ndim(x) + \".\"));\n  }\n\n  if (ndim(kernel) !== 3 && ndim(kernel) !== 4) {\n    throw new errors_1.ValueError(\"conv2dWithBias expects kernel to be of rank 3 or 4, but received \" + (ndim(x) + \".\"));\n  }\n\n  var y = preprocessConv2DInput(x, dataFormat);\n\n  if (padding === 'casual') {\n    throw new errors_1.NotImplementedError('The support for CASUAL padding mode in conv1dWithBias is not ' + 'implemented yet.');\n  }\n\n  y = tfc.conv2d(y, kernel, strides, padding === 'same' ? 'same' : 'valid', 'NHWC', dilationRate);\n\n  if (bias != null) {\n    y = biasAdd(y, bias);\n  }\n\n  if (dataFormat === 'channelsFirst') {\n    y = tfc.transpose(y, [0, 3, 1, 2]);\n  }\n\n  return y;\n}\n\nexports.conv2dWithBias = conv2dWithBias;\n\nfunction depthwiseConv2d(x, depthwiseKernel, strides, padding, dataFormat, dilationRate) {\n  if (strides === void 0) {\n    strides = [1, 1];\n  }\n\n  if (padding === void 0) {\n    padding = 'valid';\n  }\n\n  if (dataFormat == null) {\n    dataFormat = common_3.imageDataFormat();\n  }\n\n  common_1.checkDataFormat(dataFormat);\n  var y = preprocessConv2DInput(x, dataFormat);\n\n  if (ndim(x) !== 4) {\n    throw new errors_1.ValueError(\"Input for depthwiseConv2d is required to be 4-D, but is instead \" + (ndim(x) + \"-D\"));\n  }\n\n  if (ndim(depthwiseKernel) !== 4) {\n    throw new errors_1.ValueError(\"depthwiseKernel is required to be 4-D, but is instead \" + (ndim(depthwiseKernel) + \"-D\"));\n  }\n\n  y = tfc.depthwiseConv2d(y, depthwiseKernel, strides, padding === 'same' ? 'same' : 'valid', 'NHWC', dilationRate);\n\n  if (dataFormat === 'channelsFirst') {\n    y = tfc.transpose(y, [0, 3, 1, 2]);\n  }\n\n  return y;\n}\n\nexports.depthwiseConv2d = depthwiseConv2d;\n\nfunction pool2d(x, poolSize, strides, padding, dataFormat, poolMode) {\n  common_1.checkDataFormat(dataFormat);\n  common_1.checkPoolMode(poolMode);\n  common_1.checkPaddingMode(padding);\n\n  if (strides == null) {\n    strides = [1, 1];\n  }\n\n  if (padding == null) {\n    padding = 'valid';\n  }\n\n  if (dataFormat == null) {\n    dataFormat = common_3.imageDataFormat();\n  }\n\n  if (poolMode == null) {\n    poolMode = 'max';\n  }\n\n  x = preprocessConv2DInput(x, dataFormat);\n  var y;\n  var paddingString = padding === 'same' ? 'same' : 'valid';\n\n  if (poolMode === 'max') {\n    y = tfc.maxPool(x, poolSize, strides, paddingString);\n  } else {\n    y = tfc.avgPool(x, poolSize, strides, paddingString);\n  }\n\n  if (dataFormat === 'channelsFirst') {\n    y = tfc.transpose(y, [0, 3, 1, 2]);\n  }\n\n  return y;\n}\n\nexports.pool2d = pool2d;\n\nfunction nameScope(name, fn) {\n  return common_1.nameScope(name, fn);\n}\n\nexports.nameScope = nameScope;\n\nfunction floatx() {\n  return types_1.DType.float32;\n}\n\nexports.floatx = floatx;\nvar _uidPrefixes = {};\n\nfunction getUid(prefix) {\n  if (prefix === void 0) {\n    prefix = '';\n  }\n\n  if (!(prefix in _uidPrefixes)) {\n    _uidPrefixes[prefix] = 0;\n  }\n\n  _uidPrefixes[prefix] += 1;\n  return prefix + _uidPrefixes[prefix].toString();\n}\n\nexports.getUid = getUid;\n\nfunction softmax(x, axis) {\n  if (axis === void 0) {\n    axis = -1;\n  }\n\n  return tfc.softmax(x, axis);\n}\n\nexports.softmax = softmax;\n\nfunction categoricalCrossentropy(target, output, fromLogits) {\n  if (fromLogits === void 0) {\n    fromLogits = false;\n  }\n\n  if (fromLogits) {\n    output = softmax(output);\n  } else {\n    var outputSum = sum(output, shape(output).length - 1, true);\n    output = divide(output, outputSum);\n  }\n\n  output = clip(output, exports.epsilon(), 1 - exports.epsilon());\n  return tfc.neg(tfc.sum(tfc.mul(target.toFloat(), tfc.log(output)), shape(output).length - 1));\n}\n\nexports.categoricalCrossentropy = categoricalCrossentropy;\n\nfunction sparseCategoricalCrossentropy(target, output, fromLogits) {\n  if (fromLogits === void 0) {\n    fromLogits = false;\n  }\n\n  var flatTarget = tfc.floor(flatten(target)).toInt();\n  var outputShape = shape(output);\n  var oneHotTarget = reshape(tfc.oneHot(flatTarget, outputShape[outputShape.length - 1]), outputShape);\n  return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n}\n\nexports.sparseCategoricalCrossentropy = sparseCategoricalCrossentropy;\n\nfunction binaryCrossentropy(target, output, fromLogits) {\n  if (fromLogits === void 0) {\n    fromLogits = false;\n  }\n\n  var y;\n\n  if (!fromLogits) {\n    y = clip(output, exports.epsilon(), 1 - exports.epsilon());\n    y = log(divide(y, subtract(tfc.onesLike(y), y)));\n  } else {\n    y = output;\n  }\n\n  return sigmoidCrossEntropyWithLogits(target, y);\n}\n\nexports.binaryCrossentropy = binaryCrossentropy;\n\nfunction sigmoidCrossEntropyWithLogits(target, output) {\n  var maxOutput = tfc.maximum(output, tfc.zerosLike(output));\n  var outputXTarget = tfc.mul(output, target);\n  var sigmoidOutput = tfc.log(tfc.add(getScalar(1), tfc.exp(tfc.neg(tfc.abs(output)))));\n  var result = tfc.add(tfc.sub(maxOutput, outputXTarget), sigmoidOutput);\n  return result;\n}\n\nexports.sigmoidCrossEntropyWithLogits = sigmoidCrossEntropyWithLogits;\n\nfunction sigmoid(x) {\n  return tfc.sigmoid(x);\n}\n\nexports.sigmoid = sigmoid;\n\nfunction hardSigmoid(x) {\n  var y = scalarPlusArray(tfjs_core_1.scalar(0.5), scalarTimesArray(tfjs_core_1.scalar(0.2), x));\n  return clip(y, 0, 1);\n}\n\nexports.hardSigmoid = hardSigmoid;\n\nfunction inTrainPhase(x, alt, training) {\n  if (training === void 0) {\n    training = false;\n  }\n\n  return training ? x() : alt();\n}\n\nexports.inTrainPhase = inTrainPhase;\n\nfunction rnn(stepFunction, inputs, initialStates, goBackwards, mask, constants, unroll, inputLength) {\n  if (goBackwards === void 0) {\n    goBackwards = false;\n  }\n\n  if (unroll === void 0) {\n    unroll = false;\n  }\n\n  var ndim = inputs.shape.length;\n\n  if (ndim < 3) {\n    throw new errors_1.ValueError(\"Input should be at least 3D, but is \" + ndim + \"D.\");\n  }\n\n  var axes = [1, 0].concat(math_utils.range(2, ndim));\n  inputs = transpose(inputs, axes);\n\n  if (mask != null) {\n    throw new errors_1.NotImplementedError('The rnn() function of the deeplearn.js backend does not support ' + 'masking yet.');\n  }\n\n  if (constants != null) {\n    throw new errors_1.NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' + 'constants yet.');\n  }\n\n  if (unroll) {\n    console.warn('Backend rnn(): the unroll = true option is not applicable to the ' + 'imperative deeplearn.js backend.');\n  }\n\n  if (goBackwards) {\n    inputs = reverse(inputs, 0);\n  }\n\n  var outputs;\n  var lastOutput;\n  var states = initialStates;\n  var timeSteps = inputs.shape[0];\n\n  for (var t = 0; t < timeSteps; ++t) {\n    var currentInput = sliceAlongFirstAxis(inputs, t, 1);\n    currentInput = reshape(currentInput, currentInput.shape.slice(1));\n    var stepOutputs = stepFunction(currentInput, states);\n    lastOutput = stepOutputs[0];\n\n    if (t === 0) {\n      outputs = lastOutput.reshape([1].concat(lastOutput.shape));\n    } else {\n      outputs = concatAlongFirstAxis(outputs, lastOutput.reshape([1].concat(lastOutput.shape)));\n    }\n\n    states = stepOutputs[1];\n  }\n\n  return [lastOutput, transpose(outputs, [1, 0].concat(math_utils.range(2, outputs.shape.length))), states];\n}\n\nexports.rnn = rnn;\n\nfunction gradients(lossFn, variables) {\n  var variableList = variables.map(function (variable) {\n    return variable.read();\n  });\n  var valudAndGrads = tfjs_core_1.variableGrads(lossFn, variableList);\n  return variables.map(function (variable) {\n    return valudAndGrads.grads[variable.name];\n  });\n}\n\nexports.gradients = gradients;","map":null,"metadata":{},"sourceType":"script"}