{"ast":null,"code":"\"use strict\";\n\nvar __extends = this && this.__extends || function () {\n  var extendStatics = Object.setPrototypeOf || {\n    __proto__: []\n  } instanceof Array && function (d, b) {\n    d.__proto__ = b;\n  } || function (d, b) {\n    for (var p in b) {\n      if (b.hasOwnProperty(p)) d[p] = b[p];\n    }\n  };\n\n  return function (d, b) {\n    extendStatics(d, b);\n\n    function __() {\n      this.constructor = d;\n    }\n\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n  };\n}();\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\n\nvar activations_1 = require(\"../activations\");\n\nvar tfjs_backend_1 = require(\"../backend/tfjs_backend\");\n\nvar tfjs_backend_2 = require(\"../backend/tfjs_backend\");\n\nvar topology_1 = require(\"../engine/topology\");\n\nvar errors_1 = require(\"../errors\");\n\nvar types_1 = require(\"../types\");\n\nvar generic_utils = require(\"../utils/generic_utils\");\n\nvar LeakyReLU = function (_super) {\n  __extends(LeakyReLU, _super);\n\n  function LeakyReLU(config) {\n    var _this = _super.call(this, config == null ? {} : config) || this;\n\n    _this.DEFAULT_ALPHA = 0.3;\n\n    if (config == null) {\n      config = {};\n    }\n\n    _this.alpha = config.alpha == null ? _this.DEFAULT_ALPHA : config.alpha;\n    return _this;\n  }\n\n  LeakyReLU.prototype.call = function (inputs, kwargs) {\n    var x = generic_utils.getExactlyOneTensor(inputs);\n    return tfjs_core_1.leakyRelu(x, this.alpha);\n  };\n\n  LeakyReLU.prototype.computeOutputShape = function (inputShape) {\n    return inputShape;\n  };\n\n  LeakyReLU.prototype.getClassName = function () {\n    return 'LeakyReLU';\n  };\n\n  LeakyReLU.prototype.getConfig = function () {\n    var config = {\n      alpha: this.alpha\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return LeakyReLU;\n}(topology_1.Layer);\n\nexports.LeakyReLU = LeakyReLU;\ngeneric_utils.ClassNameMap.register('LeakyReLU', LeakyReLU);\n\nvar ELU = function (_super) {\n  __extends(ELU, _super);\n\n  function ELU(config) {\n    var _this = _super.call(this, config == null ? {} : config) || this;\n\n    _this.DEFAULT_ALPHA = 1.0;\n\n    if (config == null) {\n      config = {};\n    }\n\n    if (config.alpha != null && config.alpha !== _this.DEFAULT_ALPHA) {\n      throw new errors_1.NotImplementedError(\"Non-default alpha value (\" + config.alpha + \") is not supported by the \" + \"ELU layer yet.\");\n    }\n\n    _this.alpha = config.alpha == null ? _this.DEFAULT_ALPHA : config.alpha;\n    return _this;\n  }\n\n  ELU.prototype.call = function (inputs, kwargs) {\n    var x = generic_utils.getExactlyOneTensor(inputs);\n    return tfjs_core_1.elu(x);\n  };\n\n  ELU.prototype.computeOutputShape = function (inputShape) {\n    return inputShape;\n  };\n\n  ELU.prototype.getClassName = function () {\n    return 'ELU';\n  };\n\n  ELU.prototype.getConfig = function () {\n    var config = {\n      alpha: this.alpha\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return ELU;\n}(topology_1.Layer);\n\nexports.ELU = ELU;\ngeneric_utils.ClassNameMap.register('ELU', ELU);\n\nvar ThresholdedReLU = function (_super) {\n  __extends(ThresholdedReLU, _super);\n\n  function ThresholdedReLU(config) {\n    var _this = _super.call(this, config == null ? {} : config) || this;\n\n    _this.DEFAULT_THETA = 1.0;\n\n    if (config == null) {\n      config = {};\n    }\n\n    _this.theta = config.theta == null ? _this.DEFAULT_THETA : config.theta;\n    _this.thetaTensor = tfjs_backend_2.getScalar(_this.theta);\n    return _this;\n  }\n\n  ThresholdedReLU.prototype.call = function (inputs, kwargs) {\n    var x = generic_utils.getExactlyOneTensor(inputs);\n    return x.mul(tfjs_backend_1.cast(x.greater(this.thetaTensor), types_1.DType.float32));\n  };\n\n  ThresholdedReLU.prototype.computeOutputShape = function (inputShape) {\n    return inputShape;\n  };\n\n  ThresholdedReLU.prototype.getClassName = function () {\n    return 'ThresholdedReLU';\n  };\n\n  ThresholdedReLU.prototype.getConfig = function () {\n    var config = {\n      theta: this.theta\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return ThresholdedReLU;\n}(topology_1.Layer);\n\nexports.ThresholdedReLU = ThresholdedReLU;\ngeneric_utils.ClassNameMap.register('ThresholdedReLU', ThresholdedReLU);\n\nvar Softmax = function (_super) {\n  __extends(Softmax, _super);\n\n  function Softmax(config) {\n    var _this = _super.call(this, config == null ? {} : config) || this;\n\n    _this.DEFAULT_AXIS = 1.0;\n\n    if (config == null) {\n      config = {};\n    }\n\n    _this.axis = config.theta == null ? _this.DEFAULT_AXIS : config.theta;\n    return _this;\n  }\n\n  Softmax.prototype.call = function (inputs, kwargs) {\n    var x = generic_utils.getExactlyOneTensor(inputs);\n    return activations_1.softmax(x, this.axis);\n  };\n\n  Softmax.prototype.computeOutputShape = function (inputShape) {\n    return inputShape;\n  };\n\n  Softmax.prototype.getClassName = function () {\n    return 'Softmax';\n  };\n\n  Softmax.prototype.getConfig = function () {\n    var config = {\n      axis: this.axis\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return Softmax;\n}(topology_1.Layer);\n\nexports.Softmax = Softmax;\ngeneric_utils.ClassNameMap.register('Softmax', Softmax);","map":null,"metadata":{},"sourceType":"script"}