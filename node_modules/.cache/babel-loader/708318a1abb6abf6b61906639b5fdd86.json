{"ast":null,"code":"\"use strict\";\n\nvar __extends = this && this.__extends || function () {\n  var extendStatics = Object.setPrototypeOf || {\n    __proto__: []\n  } instanceof Array && function (d, b) {\n    d.__proto__ = b;\n  } || function (d, b) {\n    for (var p in b) {\n      if (b.hasOwnProperty(p)) d[p] = b[p];\n    }\n  };\n\n  return function (d, b) {\n    extendStatics(d, b);\n\n    function __() {\n      this.constructor = d;\n    }\n\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n  };\n}();\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar environment_1 = require(\"../environment\");\n\nvar globals_1 = require(\"../globals\");\n\nvar ops_1 = require(\"../ops/ops\");\n\nvar optimizer_1 = require(\"./optimizer\");\n\nvar AdadeltaOptimizer = function (_super) {\n  __extends(AdadeltaOptimizer, _super);\n\n  function AdadeltaOptimizer(learningRate, rho, epsilon) {\n    if (epsilon === void 0) {\n      epsilon = 1e-8;\n    }\n\n    var _this = _super.call(this) || this;\n\n    _this.accumulatedGrads = {};\n    _this.accumulatedUpdates = {};\n    _this.c = globals_1.keep(ops_1.scalar(-learningRate));\n    _this.epsilon = globals_1.keep(ops_1.scalar(epsilon));\n    _this.rho = globals_1.keep(ops_1.scalar(rho));\n    _this.oneMinusRho = globals_1.keep(ops_1.scalar(1 - rho));\n    return _this;\n  }\n\n  AdadeltaOptimizer.prototype.applyGradients = function (variableGradients) {\n    var _this = this;\n\n    var _loop_1 = function _loop_1(variableName) {\n      var value = environment_1.ENV.engine.registeredVariables[variableName];\n\n      if (this_1.accumulatedGrads[variableName] == null) {\n        var trainable_1 = false;\n        globals_1.tidy(function () {\n          _this.accumulatedGrads[variableName] = ops_1.zerosLike(value).variable(trainable_1);\n        });\n      }\n\n      if (this_1.accumulatedUpdates[variableName] == null) {\n        var trainable_2 = false;\n        globals_1.tidy(function () {\n          _this.accumulatedUpdates[variableName] = ops_1.zerosLike(value).variable(trainable_2);\n        });\n      }\n\n      var gradient = variableGradients[variableName];\n      var accumulatedGrad = this_1.accumulatedGrads[variableName];\n      var accumulatedUpdate = this_1.accumulatedUpdates[variableName];\n      globals_1.tidy(function () {\n        var newAccumulatedGrad = _this.rho.mul(accumulatedGrad).add(_this.oneMinusRho.mul(gradient.square()));\n\n        var updates = accumulatedUpdate.add(_this.epsilon).sqrt().div(accumulatedGrad.add(_this.epsilon).sqrt()).mul(gradient);\n\n        var newAccumulatedUpdate = _this.rho.mul(accumulatedUpdate).add(_this.oneMinusRho.mul(updates.square()));\n\n        _this.accumulatedGrads[variableName].assign(newAccumulatedGrad);\n\n        _this.accumulatedUpdates[variableName].assign(newAccumulatedUpdate);\n\n        var newValue = _this.c.mul(updates).add(value);\n\n        value.assign(newValue);\n      });\n    };\n\n    var this_1 = this;\n\n    for (var variableName in variableGradients) {\n      _loop_1(variableName);\n    }\n  };\n\n  AdadeltaOptimizer.prototype.dispose = function () {\n    var _this = this;\n\n    this.c.dispose();\n    this.epsilon.dispose();\n    this.rho.dispose();\n    this.oneMinusRho.dispose();\n\n    if (this.accumulatedUpdates != null) {\n      Object.keys(this.accumulatedUpdates).forEach(function (name) {\n        return _this.accumulatedUpdates[name].dispose();\n      });\n      Object.keys(this.accumulatedGrads).forEach(function (name) {\n        return _this.accumulatedGrads[name].dispose();\n      });\n    }\n  };\n\n  return AdadeltaOptimizer;\n}(optimizer_1.Optimizer);\n\nexports.AdadeltaOptimizer = AdadeltaOptimizer;","map":null,"metadata":{},"sourceType":"script"}