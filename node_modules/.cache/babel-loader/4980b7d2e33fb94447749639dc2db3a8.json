{"ast":null,"code":"\"use strict\";\n\nvar __extends = this && this.__extends || function () {\n  var extendStatics = Object.setPrototypeOf || {\n    __proto__: []\n  } instanceof Array && function (d, b) {\n    d.__proto__ = b;\n  } || function (d, b) {\n    for (var p in b) {\n      if (b.hasOwnProperty(p)) d[p] = b[p];\n    }\n  };\n\n  return function (d, b) {\n    extendStatics(d, b);\n\n    function __() {\n      this.constructor = d;\n    }\n\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n  };\n}();\n\nvar __decorate = this && this.__decorate || function (decorators, target, key, desc) {\n  var c = arguments.length,\n      r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc,\n      d;\n  if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);else for (var i = decorators.length - 1; i >= 0; i--) {\n    if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n  }\n  return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\n\nvar activations_1 = require(\"../activations\");\n\nvar K = require(\"../backend/tfjs_backend\");\n\nvar constraints_1 = require(\"../constraints\");\n\nvar topology_1 = require(\"../engine/topology\");\n\nvar topology_2 = require(\"../engine/topology\");\n\nvar errors_1 = require(\"../errors\");\n\nvar initializers_1 = require(\"../initializers\");\n\nvar regularizers_1 = require(\"../regularizers\");\n\nvar types_1 = require(\"../types\");\n\nvar generic_utils = require(\"../utils/generic_utils\");\n\nvar math_utils = require(\"../utils/math_utils\");\n\nvar serialization_1 = require(\"./serialization\");\n\nvar RNN = function (_super) {\n  __extends(RNN, _super);\n\n  function RNN(config) {\n    var _this = _super.call(this, config) || this;\n\n    var cell;\n\n    if (config.cell == null) {\n      throw new errors_1.ValueError('cell property is missing for the constructor of RNN.');\n    } else if (Array.isArray(config.cell)) {\n      cell = new StackedRNNCells({\n        cells: config.cell\n      });\n    } else {\n      cell = config.cell;\n    }\n\n    if (cell.stateSize == null) {\n      throw new errors_1.ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' + 'integers, one integer per RNN state).');\n    }\n\n    _this.cell = cell;\n    _this.returnSequences = config.returnSequences == null ? false : config.returnSequences;\n    _this.returnState = config.returnState == null ? false : config.returnState;\n    _this.goBackwards = config.goBackwards == null ? false : config.goBackwards;\n    _this._stateful = config.stateful == null ? false : config.stateful;\n    _this.unroll = config.unroll == null ? false : config.unroll;\n    _this.supportsMasking = true;\n    _this.inputSpec = [new topology_1.InputSpec({\n      ndim: 3\n    })];\n    _this.stateSpec = null;\n    _this.states = null;\n    _this.numConstants = null;\n    return _this;\n  }\n\n  RNN.prototype.getStates = function () {\n    if (this.states == null) {\n      var numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      return math_utils.range(0, numStates).map(function (x) {\n        return null;\n      });\n    } else {\n      return this.states;\n    }\n  };\n\n  RNN.prototype.setStates = function (states) {\n    this.states = states;\n  };\n\n  RNN.prototype.computeOutputShape = function (inputShape) {\n    if (generic_utils.isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape;\n    var stateSize = this.cell.stateSize;\n\n    if (!Array.isArray(stateSize)) {\n      stateSize = [stateSize];\n    }\n\n    var outputDim = stateSize[0];\n    var outputShape;\n\n    if (this.returnSequences) {\n      outputShape = [inputShape[0], inputShape[1], outputDim];\n    } else {\n      outputShape = [inputShape[0], outputDim];\n    }\n\n    if (this.returnState) {\n      var stateShape = [];\n\n      for (var _i = 0, stateSize_1 = stateSize; _i < stateSize_1.length; _i++) {\n        var dim = stateSize_1[_i];\n        stateShape.push([inputShape[0], dim]);\n      }\n\n      return [outputShape].concat(stateShape);\n    } else {\n      return outputShape;\n    }\n  };\n\n  RNN.prototype.computeMask = function (inputs, mask) {\n    throw new errors_1.NotImplementedError('computeMask has not been implemented for RNN yet');\n  };\n\n  RNN.prototype.build = function (inputShape) {\n    var constantShape = null;\n\n    if (this.numConstants != null) {\n      throw new errors_1.NotImplementedError('Constants support is not implemented in RNN yet.');\n    }\n\n    if (generic_utils.isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape;\n    var batchSize = this.stateful ? inputShape[0] : null;\n    var inputDim = inputShape[inputShape.length - 1];\n    this.inputSpec[0] = new topology_1.InputSpec({\n      shape: [batchSize, null, inputDim]\n    });\n    var stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n    if (constantShape != null) {\n      throw new errors_1.NotImplementedError('Constants support is not implemented in RNN yet.');\n    } else {\n      this.cell.build(stepInputShape);\n    }\n\n    var stateSize;\n\n    if (Array.isArray(this.cell.stateSize)) {\n      stateSize = this.cell.stateSize;\n    } else {\n      stateSize = [this.cell.stateSize];\n    }\n\n    if (this.stateSpec != null) {\n      if (!tfjs_core_1.util.arraysEqual(this.stateSpec.map(function (spec) {\n        return spec.shape[spec.shape.length - 1];\n      }), stateSize)) {\n        throw new errors_1.ValueError(\"An initialState was passed that is not compatible with \" + (\"cell.stateSize. Received stateSpec=\" + this.stateSpec + \"; \") + (\"However cell.stateSize is \" + this.cell.stateSize));\n      }\n    } else {\n      this.stateSpec = stateSize.map(function (dim) {\n        return new topology_1.InputSpec({\n          shape: [null, dim]\n        });\n      });\n    }\n\n    if (this.stateful) {\n      throw new errors_1.NotImplementedError('stateful RNN layer is not implemented yet');\n    }\n  };\n\n  RNN.prototype.resetStates = function (states) {\n    if (!this.stateful) {\n      throw new errors_1.AttributeError('Cannot call resetState() on an RNN Layer that is not stateful.');\n    }\n\n    var batchSize = this.inputSpec[0].shape[0];\n\n    if (batchSize == null) {\n      throw new errors_1.ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' + 'the batch size of your input tensors: \\n' + '- If using a Sequential model, specify the batch size by passing ' + 'a `batchInputShape` option to your first layer.\\n' + '- If using the functional API, specify the batch size by ' + 'passing a `batchShape` option to your Input layer.');\n    }\n\n    if (this.states == null) {\n      if (Array.isArray(this.cell.stateSize)) {\n        this.states = this.cell.stateSize.map(function (dim) {\n          return K.zeros([batchSize, dim]);\n        });\n      } else {\n        this.states = [K.zeros([batchSize, this.cell.stateSize])];\n      }\n    } else if (states == null) {\n      if (Array.isArray(this.cell.stateSize)) {\n        this.states = this.cell.stateSize.map(function (dim) {\n          return K.zeros([batchSize, dim]);\n        });\n      } else {\n        this.states[0] = K.zeros([batchSize, this.cell.stateSize]);\n      }\n    } else {\n      if (!Array.isArray(states)) {\n        states = [states];\n      }\n\n      if (states.length !== this.states.length) {\n        throw new errors_1.ValueError(\"Layer \" + this.name + \" expects \" + this.states.length + \" state(s), \" + (\"but it received \" + states.length + \" state value(s). Input \") + (\"received: \" + states));\n      }\n\n      for (var index = 0; index < this.states.length; ++index) {\n        var value = states[index];\n        var dim = Array.isArray(this.cell.stateSize) ? this.cell.stateSize[index] : this.cell.stateSize;\n        var expectedShape = [batchSize, dim];\n\n        if (!tfjs_core_1.util.arraysEqual(value.shape, expectedShape)) {\n          throw new errors_1.ValueError(\"State \" + index + \" is incompatible with layer \" + this.name + \": \" + (\"expected shape=\" + expectedShape + \", received shape=\" + value.shape));\n        }\n\n        this.states[index] = value;\n      }\n    }\n  };\n\n  RNN.prototype.standardizeArgs = function (inputs, initialState, constants) {\n    if (Array.isArray(inputs)) {\n      if (initialState != null || constants != null) {\n        throw new errors_1.ValueError('When inputs is an array, neither initialState or constants ' + 'should be provided');\n      }\n\n      if (this.numConstants != null) {\n        constants = inputs.slice(inputs.length - this.numConstants, inputs.length);\n        inputs = inputs.slice(0, inputs.length - this.numConstants);\n      }\n\n      if (inputs.length > 1) {\n        initialState = inputs.slice(1, inputs.length);\n      }\n\n      inputs = inputs[0];\n    }\n\n    function toListOrNull(x) {\n      if (x == null || Array.isArray(x)) {\n        return x;\n      } else {\n        return [x];\n      }\n    }\n\n    initialState = toListOrNull(initialState);\n    constants = toListOrNull(constants);\n    return {\n      inputs: inputs,\n      initialState: initialState,\n      constants: constants\n    };\n  };\n\n  RNN.prototype.apply = function (inputs, kwargs) {\n    var initialState = kwargs == null ? null : kwargs['initialState'];\n    var constants = kwargs == null ? null : kwargs['constants'];\n\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    var standardized = this.standardizeArgs(inputs, initialState, constants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n    var additionalInputs = [];\n    var additionalSpecs = [];\n\n    if (initialState != null) {\n      kwargs['initialState'] = initialState;\n      additionalInputs = additionalInputs.concat(initialState);\n      this.stateSpec = [];\n\n      for (var _i = 0, initialState_1 = initialState; _i < initialState_1.length; _i++) {\n        var state = initialState_1[_i];\n        this.stateSpec.push(new topology_1.InputSpec({\n          shape: state.shape\n        }));\n      }\n\n      additionalSpecs = additionalSpecs.concat(this.stateSpec);\n    }\n\n    if (constants != null) {\n      kwargs['constants'] = constants;\n      additionalInputs = additionalInputs.concat(constants);\n      this.numConstants = constants.length;\n    }\n\n    var isTensor = additionalInputs[0] instanceof types_1.SymbolicTensor;\n\n    if (isTensor) {\n      var fullInput = [inputs].concat(additionalInputs);\n      var fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      var originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n\n      var output = _super.prototype.apply.call(this, fullInput, kwargs);\n\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return _super.prototype.apply.call(this, inputs, kwargs);\n    }\n  };\n\n  RNN.prototype.call = function (inputs, kwargs) {\n    var _this = this;\n\n    var mask = kwargs == null ? null : kwargs['mask'];\n    var training = kwargs == null ? null : kwargs['training'];\n    var initialState = kwargs == null ? null : kwargs['initialState'];\n    inputs = generic_utils.getExactlyOneTensor(inputs);\n\n    if (initialState == null) {\n      if (this.stateful) {\n        throw new errors_1.NotImplementedError('stateful RNN layer is not implemented yet.');\n      } else {\n        initialState = this.getInitialState(inputs);\n      }\n    }\n\n    if (mask != null) {\n      throw new errors_1.NotImplementedError('Masking is not implemented for RNN yet');\n    }\n\n    var numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n\n    if (initialState.length !== numStates) {\n      throw new errors_1.ValueError(\"RNN Layer has \" + numStates + \" state(s) but was passed \" + (initialState.length + \" initial state(s).\"));\n    }\n\n    var inputShape = inputs.shape;\n    var timesteps = inputShape[1];\n\n    if (this.unroll) {\n      console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n    }\n\n    var cellCallKwargs = {\n      training: training\n    };\n\n    var step = function step(inputs, states) {\n      var outputs = _this.cell.call([inputs].concat(states), cellCallKwargs);\n\n      return [outputs[0], outputs.slice(1)];\n    };\n\n    var rnnOutputs = K.rnn(step, inputs, initialState, this.goBackwards, null, null, this.unroll, timesteps);\n    var lastOutput = rnnOutputs[0];\n    var outputs = rnnOutputs[1];\n    var states = rnnOutputs[2];\n\n    if (this.stateful) {\n      throw new errors_1.NotImplementedError('stateful RNN layer is not implemented yet');\n    }\n\n    var output = this.returnSequences ? outputs : lastOutput;\n\n    if (this.returnState) {\n      return [output].concat(states);\n    } else {\n      return output;\n    }\n  };\n\n  RNN.prototype.getInitialState = function (inputs) {\n    var initialState = K.zeros(inputs.shape);\n    initialState = K.sum(initialState, [1, 2]);\n    initialState = K.expandDims(initialState);\n\n    if (Array.isArray(this.cell.stateSize)) {\n      return this.cell.stateSize.map(function (dim) {\n        return dim > 1 ? K.tile(initialState, [1, dim]) : initialState;\n      });\n    } else {\n      return this.cell.stateSize > 1 ? [K.tile(initialState, [1, this.cell.stateSize])] : [initialState];\n    }\n  };\n\n  Object.defineProperty(RNN.prototype, \"trainableWeights\", {\n    get: function get() {\n      if (!this.trainable) {\n        return [];\n      }\n\n      return this.cell.trainableWeights;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(RNN.prototype, \"nonTrainableWeights\", {\n    get: function get() {\n      if (!this.trainable) {\n        return this.cell.weights;\n      }\n\n      return this.cell.nonTrainableWeights;\n    },\n    enumerable: true,\n    configurable: true\n  });\n\n  RNN.prototype.getClassName = function () {\n    return 'RNN';\n  };\n\n  RNN.prototype.getConfig = function () {\n    var config = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll\n    };\n\n    if (this.numConstants != null) {\n      config.numConstants = this.numConstants;\n    }\n\n    var cellConfig = this.cell.getConfig();\n    config.cell = {\n      className: this.cell.getClassName(),\n      config: cellConfig\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return RNN;\n}(topology_2.Layer);\n\nexports.RNN = RNN;\ngeneric_utils.ClassNameMap.register('RNN', RNN);\n\nvar RNNCell = function (_super) {\n  __extends(RNNCell, _super);\n\n  function RNNCell() {\n    return _super !== null && _super.apply(this, arguments) || this;\n  }\n\n  RNNCell = __decorate([tfjs_core_1.doc({\n    heading: 'Layers',\n    subheading: 'Classes'\n  })], RNNCell);\n  return RNNCell;\n}(topology_2.Layer);\n\nexports.RNNCell = RNNCell;\n\nvar SimpleRNNCell = function (_super) {\n  __extends(SimpleRNNCell, _super);\n\n  function SimpleRNNCell(config) {\n    var _this = _super.call(this, config) || this;\n\n    _this.DEFAULT_ACTIVATION = 'tanh';\n    _this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    _this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    _this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    _this.units = config.units;\n    _this.activation = activations_1.getActivation(config.activation == null ? _this.DEFAULT_ACTIVATION : config.activation);\n    _this.useBias = config.useBias == null ? true : config.useBias;\n    _this.kernelInitializer = initializers_1.getInitializer(config.kernelInitializer || _this.DEFAULT_KERNEL_INITIALIZER);\n    _this.recurrentInitializer = initializers_1.getInitializer(config.recurrentInitializer || _this.DEFAULT_RECURRENT_INITIALIZER);\n    _this.biasInitializer = initializers_1.getInitializer(config.biasInitializer || _this.DEFAULT_BIAS_INITIALIZER);\n    _this.kernelRegularizer = regularizers_1.getRegularizer(config.kernelRegularizer);\n    _this.recurrentRegularizer = regularizers_1.getRegularizer(config.recurrentRegularizer);\n    _this.biasRegularizer = regularizers_1.getRegularizer(config.biasRegularizer);\n    _this.kernelConstraint = constraints_1.getConstraint(config.kernelConstraint);\n    _this.recurrentConstraint = constraints_1.getConstraint(config.recurrentConstraint);\n    _this.biasConstraint = constraints_1.getConstraint(config.biasConstraint);\n    _this.dropout = math_utils.min([1, math_utils.max([0, config.dropout == null ? 0 : config.dropout])]);\n    _this.recurrentDropout = math_utils.min([1, math_utils.max([0, config.recurrentDropout == null ? 0 : config.recurrentDropout])]);\n    _this.stateSize = _this.units;\n    return _this;\n  }\n\n  SimpleRNNCell.prototype.build = function (inputShape) {\n    inputShape = generic_utils.getExactlyOneShape(inputShape);\n    this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n\n    this.built = true;\n  };\n\n  SimpleRNNCell.prototype.call = function (inputs, kwargs) {\n    inputs = inputs;\n\n    if (inputs.length !== 2) {\n      throw new errors_1.ValueError(\"SimpleRNNCell expects 2 input Tensors, got \" + inputs.length + \".\");\n    }\n\n    var prevOutput = inputs[1];\n    inputs = inputs[0];\n\n    if (this.dropout !== 0 || this.recurrentDropout !== 0) {\n      throw new errors_1.NotImplementedError('Dropout is not implemented for SimpleRNNCell yet');\n    }\n\n    var h = K.dot(inputs, this.kernel.read());\n\n    if (this.bias != null) {\n      h = K.biasAdd(h, this.bias.read());\n    }\n\n    var output = K.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n\n    if (this.activation != null) {\n      output = this.activation(output);\n    }\n\n    return [output, output];\n  };\n\n  SimpleRNNCell.prototype.getClassName = function () {\n    return 'SimpleRNNCell';\n  };\n\n  SimpleRNNCell.prototype.getConfig = function () {\n    var config = {\n      units: this.units,\n      activation: activations_1.serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: initializers_1.serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: initializers_1.serializeInitializer(this.recurrentInitializer),\n      biasInitializer: initializers_1.serializeInitializer(this.biasInitializer),\n      kernelRegularizer: regularizers_1.serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: regularizers_1.serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: regularizers_1.serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: regularizers_1.serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: constraints_1.serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: constraints_1.serializeConstraint(this.recurrentConstraint),\n      biasConstraint: constraints_1.serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return SimpleRNNCell;\n}(RNNCell);\n\nexports.SimpleRNNCell = SimpleRNNCell;\ngeneric_utils.ClassNameMap.register('SimpleRNNCell', SimpleRNNCell);\n\nvar SimpleRNN = function (_super) {\n  __extends(SimpleRNN, _super);\n\n  function SimpleRNN(config) {\n    var _this = this;\n\n    config.cell = new SimpleRNNCell(config);\n    _this = _super.call(this, config) || this;\n    return _this;\n  }\n\n  SimpleRNN.prototype.call = function (inputs, kwargs) {\n    var mask = kwargs == null ? null : kwargs['mask'];\n    var training = kwargs == null ? null : kwargs['training'];\n    var initialState = kwargs == null ? null : kwargs['initialState'];\n    return _super.prototype.call.call(this, inputs, {\n      mask: mask,\n      training: training,\n      initialState: initialState\n    });\n  };\n\n  Object.defineProperty(SimpleRNN.prototype, \"units\", {\n    get: function get() {\n      return this.cell.units;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"activation\", {\n    get: function get() {\n      return this.cell.activation;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"useBias\", {\n    get: function get() {\n      return this.cell.useBias;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"kernelInitializer\", {\n    get: function get() {\n      return this.cell.kernelInitializer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"recurrentInitializer\", {\n    get: function get() {\n      return this.cell.recurrentInitializer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"biasInitializer\", {\n    get: function get() {\n      return this.cell.biasInitializer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"kernelRegularizer\", {\n    get: function get() {\n      return this.cell.kernelRegularizer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"recurrentRegularizer\", {\n    get: function get() {\n      return this.cell.recurrentRegularizer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"biasRegularizer\", {\n    get: function get() {\n      return this.cell.biasRegularizer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"kernelConstraint\", {\n    get: function get() {\n      return this.cell.kernelConstraint;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"recurrentConstraint\", {\n    get: function get() {\n      return this.cell.recurrentConstraint;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"biasConstraint\", {\n    get: function get() {\n      return this.cell.biasConstraint;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"dropout\", {\n    get: function get() {\n      return this.cell.dropout;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(SimpleRNN.prototype, \"recurrentDropout\", {\n    get: function get() {\n      return this.cell.recurrentDropout;\n    },\n    enumerable: true,\n    configurable: true\n  });\n\n  SimpleRNN.prototype.getClassName = function () {\n    return 'SimpleRNN';\n  };\n\n  SimpleRNN.prototype.getConfig = function () {\n    var config = {\n      units: this.units,\n      activation: activations_1.serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: initializers_1.serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: initializers_1.serializeInitializer(this.recurrentInitializer),\n      biasInitializer: initializers_1.serializeInitializer(this.biasInitializer),\n      kernelRegularizer: regularizers_1.serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: regularizers_1.serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: regularizers_1.serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: regularizers_1.serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: constraints_1.serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: constraints_1.serializeConstraint(this.recurrentConstraint),\n      biasConstraint: constraints_1.serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return SimpleRNN;\n}(RNN);\n\nexports.SimpleRNN = SimpleRNN;\ngeneric_utils.ClassNameMap.register('SimpleRNN', SimpleRNN);\n\nvar GRUCell = function (_super) {\n  __extends(GRUCell, _super);\n\n  function GRUCell(config) {\n    var _this = _super.call(this, config) || this;\n\n    _this.DEFAULT_ACTIVATION = 'tanh';\n    _this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    _this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    _this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    _this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    _this.units = config.units;\n    _this.activation = activations_1.getActivation(config.activation === undefined ? _this.DEFAULT_ACTIVATION : config.activation);\n    _this.recurrentActivation = activations_1.getActivation(config.activation === undefined ? _this.DEFAULT_RECURRENT_ACTIVATION : config.recurrentActivation);\n    _this.useBias = config.useBias == null ? true : config.useBias;\n    _this.kernelInitializer = initializers_1.getInitializer(config.kernelInitializer || _this.DEFAULT_KERNEL_INITIALIZER);\n    _this.recurrentInitializer = initializers_1.getInitializer(config.recurrentInitializer || _this.DEFAULT_RECURRENT_INITIALIZER);\n    _this.biasInitializer = initializers_1.getInitializer(config.biasInitializer || _this.DEFAULT_BIAS_INITIALIZER);\n    _this.kernelRegularizer = regularizers_1.getRegularizer(config.kernelRegularizer);\n    _this.recurrentRegularizer = regularizers_1.getRegularizer(config.recurrentRegularizer);\n    _this.biasRegularizer = regularizers_1.getRegularizer(config.biasRegularizer);\n    _this.kernelConstraint = constraints_1.getConstraint(config.kernelConstraint);\n    _this.recurrentConstraint = constraints_1.getConstraint(config.recurrentConstraint);\n    _this.biasConstraint = constraints_1.getConstraint(config.biasConstraint);\n    _this.dropout = math_utils.min([1, math_utils.max([0, config.dropout == null ? 0 : config.dropout])]);\n    _this.recurrentDropout = math_utils.min([1, math_utils.max([0, config.recurrentDropout == null ? 0 : config.recurrentDropout])]);\n    _this.implementation = config.implementation;\n    _this.stateSize = _this.units;\n    return _this;\n  }\n\n  GRUCell.prototype.build = function (inputShape) {\n    inputShape = generic_utils.getExactlyOneShape(inputShape);\n    var inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n    if (this.useBias) {\n      this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n\n    this.built = true;\n  };\n\n  GRUCell.prototype.call = function (inputs, kwargs) {\n    if (this.dropout !== 0 || this.recurrentDropout !== 0) {\n      throw new errors_1.NotImplementedError('Dropout is not implemented for GRUCell yet');\n    }\n\n    inputs = inputs;\n\n    if (inputs.length !== 2) {\n      throw new errors_1.ValueError(\"GRUCell expects 2 input Tensors (inputs, h, c), got \" + (inputs.length + \".\"));\n    }\n\n    var hTMinus1 = inputs[1];\n    inputs = inputs[0];\n    var z;\n    var r;\n    var hh;\n\n    if (this.implementation === 1) {\n      var kernelZ = K.sliceAlongLastAxis(this.kernel.read(), 0, this.units);\n      var kernelR = K.sliceAlongLastAxis(this.kernel.read(), this.units, this.units);\n      var kernelH = K.sliceAlongLastAxis(this.kernel.read(), this.units * 2, this.units);\n      var recurrentKernelZ = K.sliceAlongLastAxis(this.recurrentKernel.read(), 0, this.units);\n      var recurrentKernelR = K.sliceAlongLastAxis(this.recurrentKernel.read(), this.units, this.units);\n      var recurrentKernelH = K.sliceAlongLastAxis(this.recurrentKernel.read(), this.units * 2, this.units);\n      var inputsZ = inputs;\n      var inputsR = inputs;\n      var inputsH = inputs;\n      var xZ = K.dot(inputsZ, kernelZ);\n      var xR = K.dot(inputsR, kernelR);\n      var xH = K.dot(inputsH, kernelH);\n\n      if (this.useBias) {\n        var biasZ = K.sliceAlongFirstAxis(this.bias.read(), 0, this.units);\n        var biasR = K.sliceAlongFirstAxis(this.bias.read(), this.units, this.units);\n        var biasH = K.sliceAlongFirstAxis(this.bias.read(), this.units * 2, this.units);\n        xZ = K.biasAdd(xZ, biasZ);\n        xR = K.biasAdd(xR, biasR);\n        xH = K.biasAdd(xH, biasH);\n      }\n\n      var hTMinus1Z = hTMinus1;\n      var hTMinus1R = hTMinus1;\n      var hTMinus1H = hTMinus1;\n      z = this.recurrentActivation(K.add(xZ, K.dot(hTMinus1Z, recurrentKernelZ)));\n      r = this.recurrentActivation(K.add(xR, K.dot(hTMinus1R, recurrentKernelR)));\n      hh = this.activation(K.add(xH, K.dot(K.multiply(r, hTMinus1H), recurrentKernelH)));\n    } else {\n      var matrixX = K.dot(inputs, this.kernel.read());\n\n      if (this.useBias) {\n        matrixX = K.biasAdd(matrixX, this.bias.read());\n      }\n\n      var matrixInner = K.dot(hTMinus1, K.sliceAlongLastAxis(this.recurrentKernel.read(), 0, 2 * this.units));\n      var xZ = K.sliceAlongLastAxis(matrixX, 0, this.units);\n      var xR = K.sliceAlongLastAxis(matrixX, this.units, this.units);\n      var recurrentZ = K.sliceAlongLastAxis(matrixInner, 0, this.units);\n      var recurrentR = K.sliceAlongLastAxis(matrixInner, this.units, this.units);\n      z = this.recurrentActivation(K.add(xZ, recurrentZ));\n      r = this.recurrentActivation(K.add(xR, recurrentR));\n      var xH = K.sliceAlongLastAxis(matrixX, 2 * this.units, this.units);\n      var recurrentH = K.dot(K.multiply(r, hTMinus1), K.sliceAlongLastAxis(this.recurrentKernel.read(), 2 * this.units, this.units));\n      hh = this.activation(K.add(xH, recurrentH));\n    }\n\n    var h = K.add(K.multiply(z, hTMinus1), K.multiply(K.scalarPlusArray(K.getScalar(1), K.neg(z)), hh));\n    return [h, h];\n  };\n\n  GRUCell.prototype.getClassName = function () {\n    return 'GRUCell';\n  };\n\n  GRUCell.prototype.getConfig = function () {\n    var config = {\n      units: this.units,\n      activation: activations_1.serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: initializers_1.serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: initializers_1.serializeInitializer(this.recurrentInitializer),\n      biasInitializer: initializers_1.serializeInitializer(this.biasInitializer),\n      kernelRegularizer: regularizers_1.serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: regularizers_1.serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: regularizers_1.serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: regularizers_1.serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: constraints_1.serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: constraints_1.serializeConstraint(this.recurrentConstraint),\n      biasConstraint: constraints_1.serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return GRUCell;\n}(RNNCell);\n\nexports.GRUCell = GRUCell;\ngeneric_utils.ClassNameMap.register('GRUCell', GRUCell);\n\nvar GRU = function (_super) {\n  __extends(GRU, _super);\n\n  function GRU(config) {\n    var _this = this;\n\n    if (config.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    config.cell = new GRUCell(config);\n    _this = _super.call(this, config) || this;\n    return _this;\n  }\n\n  GRU.prototype.call = function (inputs, kwargs) {\n    var mask = kwargs == null ? null : kwargs['mask'];\n    var training = kwargs == null ? null : kwargs['training'];\n    var initialState = kwargs == null ? null : kwargs['initialState'];\n    return _super.prototype.call.call(this, inputs, {\n      mask: mask,\n      training: training,\n      initialState: initialState\n    });\n  };\n\n  Object.defineProperty(GRU.prototype, \"units\", {\n    get: function get() {\n      return this.cell.units;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"activation\", {\n    get: function get() {\n      return this.cell.activation;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"useBias\", {\n    get: function get() {\n      return this.cell.useBias;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"kernelInitializer\", {\n    get: function get() {\n      return this.cell.kernelInitializer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"recurrentInitializer\", {\n    get: function get() {\n      return this.cell.recurrentInitializer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"biasInitializer\", {\n    get: function get() {\n      return this.cell.biasInitializer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"kernelRegularizer\", {\n    get: function get() {\n      return this.cell.kernelRegularizer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"recurrentRegularizer\", {\n    get: function get() {\n      return this.cell.recurrentRegularizer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"biasRegularizer\", {\n    get: function get() {\n      return this.cell.biasRegularizer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"kernelConstraint\", {\n    get: function get() {\n      return this.cell.kernelConstraint;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"recurrentConstraint\", {\n    get: function get() {\n      return this.cell.recurrentConstraint;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"biasConstraint\", {\n    get: function get() {\n      return this.cell.biasConstraint;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"dropout\", {\n    get: function get() {\n      return this.cell.dropout;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"recurrentDropout\", {\n    get: function get() {\n      return this.cell.recurrentDropout;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(GRU.prototype, \"implementation\", {\n    get: function get() {\n      return this.cell.implementation;\n    },\n    enumerable: true,\n    configurable: true\n  });\n\n  GRU.prototype.getClassName = function () {\n    return 'GRU';\n  };\n\n  GRU.prototype.getConfig = function () {\n    var config = {\n      units: this.units,\n      activation: activations_1.serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: initializers_1.serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: initializers_1.serializeInitializer(this.recurrentInitializer),\n      biasInitializer: initializers_1.serializeInitializer(this.biasInitializer),\n      kernelRegularizer: regularizers_1.serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: regularizers_1.serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: regularizers_1.serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: regularizers_1.serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: constraints_1.serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: constraints_1.serializeConstraint(this.recurrentConstraint),\n      biasConstraint: constraints_1.serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  GRU.fromConfig = function (cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n\n    return new cls(config);\n  };\n\n  return GRU;\n}(RNN);\n\nexports.GRU = GRU;\ngeneric_utils.ClassNameMap.register('GRU', GRU);\n\nvar LSTMCell = function (_super) {\n  __extends(LSTMCell, _super);\n\n  function LSTMCell(config) {\n    var _this = _super.call(this, config) || this;\n\n    _this.DEFAULT_ACTIVATION = 'tanh';\n    _this.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    _this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    _this.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    _this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    _this.units = config.units;\n    _this.activation = activations_1.getActivation(config.activation === undefined ? _this.DEFAULT_ACTIVATION : config.activation);\n    _this.recurrentActivation = activations_1.getActivation(config.activation === undefined ? _this.DEFAULT_RECURRENT_ACTIVATION : config.recurrentActivation);\n    _this.useBias = config.useBias == null ? true : config.useBias;\n    _this.kernelInitializer = initializers_1.getInitializer(config.kernelInitializer || _this.DEFAULT_KERNEL_INITIALIZER);\n    _this.recurrentInitializer = initializers_1.getInitializer(config.recurrentInitializer || _this.DEFAULT_RECURRENT_INITIALIZER);\n    _this.biasInitializer = initializers_1.getInitializer(config.biasInitializer || _this.DEFAULT_BIAS_INITIALIZER);\n    _this.unitForgetBias = config.unitForgetBias;\n    _this.kernelRegularizer = regularizers_1.getRegularizer(config.kernelRegularizer);\n    _this.recurrentRegularizer = regularizers_1.getRegularizer(config.recurrentRegularizer);\n    _this.biasRegularizer = regularizers_1.getRegularizer(config.biasRegularizer);\n    _this.kernelConstraint = constraints_1.getConstraint(config.kernelConstraint);\n    _this.recurrentConstraint = constraints_1.getConstraint(config.recurrentConstraint);\n    _this.biasConstraint = constraints_1.getConstraint(config.biasConstraint);\n    _this.dropout = math_utils.min([1, math_utils.max([0, config.dropout == null ? 0 : config.dropout])]);\n    _this.recurrentDropout = math_utils.min([1, math_utils.max([0, config.recurrentDropout == null ? 0 : config.recurrentDropout])]);\n    _this.implementation = config.implementation;\n    _this.stateSize = [_this.units, _this.units];\n    return _this;\n  }\n\n  LSTMCell.prototype.build = function (inputShape) {\n    inputShape = generic_utils.getExactlyOneShape(inputShape);\n    var inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n    var biasInitializer;\n\n    if (this.useBias) {\n      if (this.unitForgetBias) {\n        var capturedBiasInit_1 = this.biasInitializer;\n        var capturedUnits_1 = this.units;\n        biasInitializer = new (function (_super) {\n          __extends(CustomInit, _super);\n\n          function CustomInit() {\n            return _super !== null && _super.apply(this, arguments) || this;\n          }\n\n          CustomInit.prototype.apply = function (shape, dtype) {\n            var bI = capturedBiasInit_1.apply([capturedUnits_1]);\n            var bF = new initializers_1.Ones().apply([capturedUnits_1]);\n            var bCAndH = capturedBiasInit_1.apply([capturedUnits_1 * 2]);\n            return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n          };\n\n          CustomInit.prototype.getClassName = function () {\n            return 'CustomInit';\n          };\n\n          return CustomInit;\n        }(initializers_1.Initializer))();\n      } else {\n        biasInitializer = this.biasInitializer;\n      }\n\n      this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n\n    this.built = true;\n  };\n\n  LSTMCell.prototype.call = function (inputs, kwargs) {\n    if (this.dropout !== 0 || this.recurrentDropout !== 0) {\n      throw new errors_1.NotImplementedError('Dropout is not implemented for LSTMCell yet');\n    }\n\n    inputs = inputs;\n\n    if (inputs.length !== 3) {\n      throw new errors_1.ValueError(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \" + (inputs.length + \".\"));\n    }\n\n    var hTMinus1 = inputs[1];\n    var cTMinus1 = inputs[2];\n    inputs = inputs[0];\n    var i;\n    var f;\n    var c;\n    var o;\n\n    if (this.implementation === 1) {\n      var kernelI = K.sliceAlongLastAxis(this.kernel.read(), 0, this.units);\n      var kernelF = K.sliceAlongLastAxis(this.kernel.read(), this.units, this.units);\n      var kernelC = K.sliceAlongLastAxis(this.kernel.read(), this.units * 2, this.units);\n      var kernelO = K.sliceAlongLastAxis(this.kernel.read(), this.units * 3, this.units);\n      var recurrentKernelI = K.sliceAlongLastAxis(this.recurrentKernel.read(), 0, this.units);\n      var recurrentKernelF = K.sliceAlongLastAxis(this.recurrentKernel.read(), this.units, this.units);\n      var recurrentKernelC = K.sliceAlongLastAxis(this.recurrentKernel.read(), this.units * 2, this.units);\n      var recurrentKernelO = K.sliceAlongLastAxis(this.recurrentKernel.read(), this.units * 3, this.units);\n      var inputsI = inputs;\n      var inputsF = inputs;\n      var inputsC = inputs;\n      var inputsO = inputs;\n      var xI = K.dot(inputsI, kernelI);\n      var xF = K.dot(inputsF, kernelF);\n      var xC = K.dot(inputsC, kernelC);\n      var xO = K.dot(inputsO, kernelO);\n\n      if (this.useBias) {\n        var biasI = K.sliceAlongFirstAxis(this.bias.read(), 0, this.units);\n        var biasF = K.sliceAlongFirstAxis(this.bias.read(), this.units, this.units);\n        var biasC = K.sliceAlongFirstAxis(this.bias.read(), this.units * 2, this.units);\n        var biasO = K.sliceAlongFirstAxis(this.bias.read(), this.units * 3, this.units);\n        xI = K.biasAdd(xI, biasI);\n        xF = K.biasAdd(xF, biasF);\n        xC = K.biasAdd(xC, biasC);\n        xO = K.biasAdd(xO, biasO);\n      }\n\n      var hTMinus1I = hTMinus1;\n      var hTMinus1F = hTMinus1;\n      var hTMinus1C = hTMinus1;\n      var hTMinus1O = hTMinus1;\n      i = this.recurrentActivation(K.add(xI, K.dot(hTMinus1I, recurrentKernelI)));\n      f = this.recurrentActivation(K.add(xF, K.dot(hTMinus1F, recurrentKernelF)));\n      c = K.add(K.multiply(f, cTMinus1), K.multiply(i, this.activation(K.add(xC, K.dot(hTMinus1C, recurrentKernelC)))));\n      o = this.recurrentActivation(K.add(xO, K.dot(hTMinus1O, recurrentKernelO)));\n    } else {\n      var z = K.dot(inputs, this.kernel.read());\n      z = K.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n\n      if (this.useBias) {\n        z = K.biasAdd(z, this.bias.read());\n      }\n\n      var z0 = K.sliceAlongLastAxis(z, 0, this.units);\n      var z1 = K.sliceAlongLastAxis(z, this.units, this.units);\n      var z2 = K.sliceAlongLastAxis(z, this.units * 2, this.units);\n      var z3 = K.sliceAlongLastAxis(z, this.units * 3, this.units);\n      i = this.recurrentActivation(z0);\n      f = this.recurrentActivation(z1);\n      c = K.add(K.multiply(f, cTMinus1), K.multiply(i, this.activation(z2)));\n      o = this.recurrentActivation(z3);\n    }\n\n    var h = K.multiply(o, this.activation(c));\n    return [h, h, c];\n  };\n\n  LSTMCell.prototype.getClassName = function () {\n    return 'LSTMCell';\n  };\n\n  LSTMCell.prototype.getConfig = function () {\n    var config = {\n      units: this.units,\n      activation: activations_1.serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: initializers_1.serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: initializers_1.serializeInitializer(this.recurrentInitializer),\n      biasInitializer: initializers_1.serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: regularizers_1.serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: regularizers_1.serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: regularizers_1.serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: regularizers_1.serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: constraints_1.serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: constraints_1.serializeConstraint(this.recurrentConstraint),\n      biasConstraint: constraints_1.serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return LSTMCell;\n}(RNNCell);\n\nexports.LSTMCell = LSTMCell;\ngeneric_utils.ClassNameMap.register('LSTMCell', LSTMCell);\n\nvar LSTM = function (_super) {\n  __extends(LSTM, _super);\n\n  function LSTM(config) {\n    var _this = this;\n\n    if (config.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    config.cell = new LSTMCell(config);\n    _this = _super.call(this, config) || this;\n    return _this;\n  }\n\n  LSTM.prototype.call = function (inputs, kwargs) {\n    var mask = kwargs == null ? null : kwargs['mask'];\n    var training = kwargs == null ? null : kwargs['training'];\n    var initialState = kwargs == null ? null : kwargs['initialState'];\n    return _super.prototype.call.call(this, inputs, {\n      mask: mask,\n      training: training,\n      initialState: initialState\n    });\n  };\n\n  Object.defineProperty(LSTM.prototype, \"units\", {\n    get: function get() {\n      return this.cell.units;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"activation\", {\n    get: function get() {\n      return this.cell.activation;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"useBias\", {\n    get: function get() {\n      return this.cell.useBias;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"kernelInitializer\", {\n    get: function get() {\n      return this.cell.kernelInitializer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"recurrentInitializer\", {\n    get: function get() {\n      return this.cell.recurrentInitializer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"biasInitializer\", {\n    get: function get() {\n      return this.cell.biasInitializer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"unitForgetBias\", {\n    get: function get() {\n      return this.cell.unitForgetBias;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"kernelRegularizer\", {\n    get: function get() {\n      return this.cell.kernelRegularizer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"recurrentRegularizer\", {\n    get: function get() {\n      return this.cell.recurrentRegularizer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"biasRegularizer\", {\n    get: function get() {\n      return this.cell.biasRegularizer;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"kernelConstraint\", {\n    get: function get() {\n      return this.cell.kernelConstraint;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"recurrentConstraint\", {\n    get: function get() {\n      return this.cell.recurrentConstraint;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"biasConstraint\", {\n    get: function get() {\n      return this.cell.biasConstraint;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"dropout\", {\n    get: function get() {\n      return this.cell.dropout;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"recurrentDropout\", {\n    get: function get() {\n      return this.cell.recurrentDropout;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(LSTM.prototype, \"implementation\", {\n    get: function get() {\n      return this.cell.implementation;\n    },\n    enumerable: true,\n    configurable: true\n  });\n\n  LSTM.prototype.getClassName = function () {\n    return 'LSTM';\n  };\n\n  LSTM.prototype.getConfig = function () {\n    var config = {\n      units: this.units,\n      activation: activations_1.serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: initializers_1.serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: initializers_1.serializeInitializer(this.recurrentInitializer),\n      biasInitializer: initializers_1.serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: regularizers_1.serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: regularizers_1.serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: regularizers_1.serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: regularizers_1.serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: constraints_1.serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: constraints_1.serializeConstraint(this.recurrentConstraint),\n      biasConstraint: constraints_1.serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  LSTM.fromConfig = function (cls, config) {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n\n    return new cls(config);\n  };\n\n  return LSTM;\n}(RNN);\n\nexports.LSTM = LSTM;\ngeneric_utils.ClassNameMap.register('LSTM', LSTM);\n\nvar StackedRNNCells = function (_super) {\n  __extends(StackedRNNCells, _super);\n\n  function StackedRNNCells(config) {\n    var _this = _super.call(this, config) || this;\n\n    _this.cells = config.cells;\n    return _this;\n  }\n\n  Object.defineProperty(StackedRNNCells.prototype, \"stateSize\", {\n    get: function get() {\n      var stateSize = [];\n\n      for (var _i = 0, _a = this.cells.slice().reverse(); _i < _a.length; _i++) {\n        var cell = _a[_i];\n\n        if (Array.isArray(cell.stateSize)) {\n          stateSize.push.apply(stateSize, cell.stateSize);\n        } else {\n          stateSize.push(cell.stateSize);\n        }\n      }\n\n      return stateSize;\n    },\n    enumerable: true,\n    configurable: true\n  });\n\n  StackedRNNCells.prototype.call = function (inputs, kwargs) {\n    inputs = inputs;\n    var states = inputs.slice(1);\n    var nestedStates = [];\n\n    for (var _i = 0, _a = this.cells.slice().reverse(); _i < _a.length; _i++) {\n      var cell = _a[_i];\n\n      if (Array.isArray(cell.stateSize)) {\n        nestedStates.push(states.splice(0, cell.stateSize.length));\n      } else {\n        nestedStates.push(states.splice(0, 1));\n      }\n    }\n\n    nestedStates.reverse();\n    var newNestedStates = [];\n    var callInputs;\n\n    for (var i = 0; i < this.cells.length; ++i) {\n      var cell = this.cells[i];\n      states = nestedStates[i];\n\n      if (i === 0) {\n        callInputs = [inputs[0]].concat(states);\n      } else {\n        callInputs = [callInputs[0]].concat(states);\n      }\n\n      callInputs = cell.call(callInputs, kwargs);\n      newNestedStates.push(callInputs.slice(1));\n    }\n\n    states = [];\n\n    for (var _b = 0, _c = newNestedStates.slice().reverse(); _b < _c.length; _b++) {\n      var cellStates = _c[_b];\n      states.push.apply(states, cellStates);\n    }\n\n    return [callInputs[0]].concat(states);\n  };\n\n  StackedRNNCells.prototype.build = function (inputShape) {\n    if (generic_utils.isArrayOfShapes(inputShape)) {\n      inputShape = inputShape[0];\n    }\n\n    inputShape = inputShape;\n    var outputDim;\n\n    for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n      var cell = _a[_i];\n      cell.build(inputShape);\n\n      if (Array.isArray(cell.stateSize)) {\n        outputDim = cell.stateSize[0];\n      } else {\n        outputDim = cell.stateSize;\n      }\n\n      inputShape = [inputShape[0], outputDim];\n    }\n\n    this.built = true;\n  };\n\n  StackedRNNCells.prototype.getClassName = function () {\n    return 'StackedRNNCells';\n  };\n\n  StackedRNNCells.prototype.getConfig = function () {\n    var cellConfigs = [];\n\n    for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n      var cell = _a[_i];\n      cellConfigs.push({\n        'className': this.getClassName(),\n        'config': cell.getConfig()\n      });\n    }\n\n    var config = {\n      'cells': cellConfigs\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  StackedRNNCells.fromConfig = function (cls, config, customObjects) {\n    if (customObjects === void 0) {\n      customObjects = {};\n    }\n\n    var cells = [];\n\n    for (var _i = 0, _a = config['cells']; _i < _a.length; _i++) {\n      var cellConfig = _a[_i];\n      cells.push(serialization_1.deserialize(cellConfig, customObjects));\n    }\n\n    return new cls({\n      cells: cells\n    });\n  };\n\n  Object.defineProperty(StackedRNNCells.prototype, \"trainableWeights\", {\n    get: function get() {\n      if (!this.trainable) {\n        return [];\n      }\n\n      var weights = [];\n\n      for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n        var cell = _a[_i];\n        weights.push.apply(weights, cell.trainableWeights);\n      }\n\n      return weights;\n    },\n    enumerable: true,\n    configurable: true\n  });\n  Object.defineProperty(StackedRNNCells.prototype, \"nonTrainableWeights\", {\n    get: function get() {\n      var weights = [];\n\n      for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n        var cell = _a[_i];\n        weights.push.apply(weights, cell.nonTrainableWeights);\n      }\n\n      if (!this.trainable) {\n        var trainableWeights = [];\n\n        for (var _b = 0, _c = this.cells; _b < _c.length; _b++) {\n          var cell = _c[_b];\n          trainableWeights.push.apply(trainableWeights, cell.trainableWeights);\n        }\n\n        return trainableWeights.concat(weights);\n      }\n\n      return weights;\n    },\n    enumerable: true,\n    configurable: true\n  });\n\n  StackedRNNCells.prototype.getWeights = function () {\n    var weights = [];\n\n    for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n      var cell = _a[_i];\n      weights.push.apply(weights, cell.weights);\n    }\n\n    return K.batchGetValue(weights);\n  };\n\n  StackedRNNCells.prototype.setWeights = function (weights) {\n    var tuples = [];\n\n    for (var _i = 0, _a = this.cells; _i < _a.length; _i++) {\n      var cell = _a[_i];\n      var numParams = cell.weights.length;\n      var inputWeights = weights.splice(numParams);\n\n      for (var i = 0; i < cell.weights.length; ++i) {\n        tuples.push([cell.weights[i], inputWeights[i]]);\n      }\n    }\n\n    K.batchSetValue(tuples);\n  };\n\n  return StackedRNNCells;\n}(RNNCell);\n\nexports.StackedRNNCells = StackedRNNCells;\ngeneric_utils.ClassNameMap.register('StackedRNNCells', StackedRNNCells);","map":null,"metadata":{},"sourceType":"script"}