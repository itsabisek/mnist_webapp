{"ast":null,"code":"\"use strict\";\n\nvar __extends = this && this.__extends || function () {\n  var extendStatics = Object.setPrototypeOf || {\n    __proto__: []\n  } instanceof Array && function (d, b) {\n    d.__proto__ = b;\n  } || function (d, b) {\n    for (var p in b) {\n      if (b.hasOwnProperty(p)) d[p] = b[p];\n    }\n  };\n\n  return function (d, b) {\n    extendStatics(d, b);\n\n    function __() {\n      this.constructor = d;\n    }\n\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n  };\n}();\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\n\nvar K = require(\"../backend/tfjs_backend\");\n\nvar constraints_1 = require(\"../constraints\");\n\nvar topology_1 = require(\"../engine/topology\");\n\nvar errors_1 = require(\"../errors\");\n\nvar initializers_1 = require(\"../initializers\");\n\nvar regularizers_1 = require(\"../regularizers\");\n\nvar generic_utils = require(\"../utils/generic_utils\");\n\nvar math_utils_1 = require(\"../utils/math_utils\");\n\nvar BatchNormalization = function (_super) {\n  __extends(BatchNormalization, _super);\n\n  function BatchNormalization(config) {\n    var _this = _super.call(this, config) || this;\n\n    _this.supportsMasking = true;\n    _this.axis = config.axis == null ? -1 : config.axis;\n    _this.momentum = config.momentum == null ? 0.99 : config.momentum;\n    _this.epsilon = config.epsilon == null ? 1e-3 : config.epsilon;\n    _this.center = config.center == null ? true : config.center;\n    _this.scale = config.scale == null ? true : config.scale;\n    _this.betaInitializer = initializers_1.getInitializer(config.betaInitializer || 'zeros');\n    _this.gammaInitializer = initializers_1.getInitializer(config.gammaInitializer || 'ones');\n    _this.movingMeanInitializer = initializers_1.getInitializer(config.movingMeanInitializer || 'zeros');\n    _this.movingVarianceInitializer = initializers_1.getInitializer(config.movingVarianceInitializer || 'ones');\n    _this.betaConstraint = constraints_1.getConstraint(config.betaConstraint);\n    _this.gammaConstraint = constraints_1.getConstraint(config.gammaConstraint);\n    _this.betaRegularizer = regularizers_1.getRegularizer(config.betaRegularizer);\n    _this.gammaRegularizer = regularizers_1.getRegularizer(config.gammaRegularizer);\n    _this.stepCount = 0;\n    return _this;\n  }\n\n  BatchNormalization.prototype.build = function (inputShape) {\n    inputShape = generic_utils.getExactlyOneShape(inputShape);\n    var axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;\n    var dim = inputShape[axis];\n\n    if (dim == null) {\n      throw new errors_1.ValueError(\"Axis \" + axis + \" of input tensor should have a defined dimension but \" + \"the layer received an input with shape \" + (JSON.stringify(inputShape) + \".\"));\n    }\n\n    this.inputSpec = [new topology_1.InputSpec({\n      ndim: inputShape.length,\n      axes: (_a = {}, _a[axis] = dim, _a)\n    })];\n    var shape = [dim];\n\n    if (this.scale) {\n      this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n    }\n\n    if (this.center) {\n      this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n    }\n\n    this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n    this.built = true;\n\n    var _a;\n  };\n\n  BatchNormalization.prototype.call = function (inputs, kwargs) {\n    var _this = this;\n\n    return tfjs_core_1.tidy(function () {\n      var training = kwargs['training'] == null ? false : kwargs['training'];\n      var input = generic_utils.getExactlyOneTensor(inputs);\n      var inputShape = K.shape(input);\n      var ndim = inputShape.length;\n      var reductionAxes = math_utils_1.range(0, ndim);\n      var axis = _this.axis >= 0 ? _this.axis : _this.axis + ndim;\n      reductionAxes.splice(axis, 1);\n      var broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n      var sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      var needsBroadcasting = !tfjs_core_1.util.arraysEqual(sortedReductionAxes, math_utils_1.range(0, ndim).slice(0, ndim - 1));\n\n      var normalizeInference = function normalizeInference() {\n        if (needsBroadcasting) {\n          var broadcastMovingMean = K.reshape(_this.movingMean.read(), broadcastShape);\n          var broadcastMovingVariance = K.reshape(_this.movingVariance.read(), broadcastShape);\n          var broadcastBeta = _this.center ? K.reshape(_this.beta.read(), broadcastShape) : null;\n          var broadcastGamma = _this.scale ? K.reshape(_this.gamma.read(), broadcastShape) : null;\n          return K.batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, _this.epsilon);\n        } else {\n          return K.batchNormalization(input, _this.movingMean.read(), _this.movingVariance.read(), _this.beta == null ? null : _this.beta.read(), _this.gamma == null ? null : _this.gamma.read(), _this.epsilon);\n        }\n      };\n\n      if (!training) {\n        return normalizeInference();\n      }\n\n      var _a = K.normalizeBatchInTraining(input, _this.gamma.read(), _this.beta.read(), reductionAxes, _this.epsilon),\n          normedTraining = _a[0],\n          mean = _a[1],\n          variance = _a[2];\n\n      var sampleSize = math_utils_1.arrayProd(reductionAxes.map(function (axis) {\n        return input.shape[axis];\n      }));\n      var varianceDebiased = variance.mul(K.getScalar(sampleSize / (sampleSize - (1 + _this.epsilon))));\n\n      var updateMovingMeanAndVariance = function updateMovingMeanAndVariance() {\n        _this.stepCount++;\n        var newMovingMean = tfjs_core_1.movingAverage(_this.movingMean.read(), mean, _this.momentum, _this.stepCount);\n\n        _this.movingMean.write(newMovingMean);\n\n        var newMovingVariance = tfjs_core_1.movingAverage(_this.movingVariance.read(), varianceDebiased, _this.momentum, _this.stepCount);\n\n        _this.movingVariance.write(newMovingVariance);\n      };\n\n      updateMovingMeanAndVariance();\n      return normedTraining;\n    });\n  };\n\n  BatchNormalization.prototype.getClassName = function () {\n    return 'BatchNormalization';\n  };\n\n  BatchNormalization.prototype.getConfig = function () {\n    var config = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: initializers_1.serializeInitializer(this.betaInitializer),\n      gammaInitializer: initializers_1.serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: initializers_1.serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer: initializers_1.serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: regularizers_1.serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: regularizers_1.serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: constraints_1.serializeConstraint(this.betaConstraint),\n      gammaConstraint: constraints_1.serializeConstraint(this.gammaConstraint)\n    };\n\n    var baseConfig = _super.prototype.getConfig.call(this);\n\n    Object.assign(config, baseConfig);\n    return config;\n  };\n\n  return BatchNormalization;\n}(topology_1.Layer);\n\nexports.BatchNormalization = BatchNormalization;\ngeneric_utils.ClassNameMap.register('BatchNormalization', BatchNormalization);","map":null,"metadata":{},"sourceType":"script"}