{"ast":null,"code":"\"use strict\";\n\nvar __decorate = this && this.__decorate || function (decorators, target, key, desc) {\n  var c = arguments.length,\n      r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc,\n      d;\n  if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);else for (var i = decorators.length - 1; i >= 0; i--) {\n    if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n  }\n  return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar doc_1 = require(\"../doc\");\n\nvar globals_1 = require(\"../globals\");\n\nvar util = require(\"../util\");\n\nvar axis_util = require(\"./axis_util\");\n\nvar operation_1 = require(\"./operation\");\n\nvar ops = require(\"./ops\");\n\nvar SoftmaxOps = function () {\n  function SoftmaxOps() {}\n\n  SoftmaxOps.softmax = function (logits, dim) {\n    if (dim === void 0) {\n      dim = -1;\n    }\n\n    util.assertArgumentsAreTensors({\n      logits: logits\n    }, 'softmax');\n\n    if (dim === -1) {\n      dim = logits.rank - 1;\n    }\n\n    if (dim !== logits.rank - 1) {\n      throw Error('Softmax along a non-last dimension is not yet supported. ' + (\"Logits was rank \" + logits.rank + \" and dim was \" + dim));\n    }\n\n    var customOp = globals_1.customGrad(function (logits) {\n      var keepDims = true;\n      var lse = logits.logSumExp([dim], keepDims);\n      var logResult = logits.toFloat().sub(lse);\n      var y = logResult.exp();\n\n      var gradFunc = function gradFunc(dy) {\n        var dyTimesY = dy.mul(y);\n        var keepDims = true;\n        return dyTimesY.sub(dyTimesY.sum([dim], keepDims).mul(y));\n      };\n\n      return {\n        value: y,\n        gradFunc: gradFunc\n      };\n    });\n    return customOp(logits);\n  };\n\n  SoftmaxOps.softmaxCrossEntropy = function (labels, logits, dim) {\n    if (dim === void 0) {\n      dim = -1;\n    }\n\n    util.assertArgumentsAreTensors({\n      labels: labels,\n      logits: logits\n    }, 'softmaxCrossEntropy');\n    util.assertShapesMatch(labels.shape, logits.shape, 'Error in softmaxCrossEntropy: ');\n\n    if (dim === -1) {\n      dim = logits.rank - 1;\n    }\n\n    if (dim !== logits.rank - 1) {\n      throw Error(\"Softmax cross entropy along a non-last dimension is not yet \" + (\"supported. Labels / logits was rank \" + logits.rank + \" \") + (\"and dim was \" + dim));\n    }\n\n    var customOp = globals_1.customGrad(function (labels, logits) {\n      var predictedProbs = logits.softmax(dim);\n      var costVector = ops.scalar(1e-5).add(predictedProbs).log().mul(labels).neg();\n      var value = costVector.sum([dim]);\n\n      var gradFunc = function gradFunc(dy) {\n        var dyShape = axis_util.expandShapeToKeepDim(dy.shape, [dim]);\n        return [dy.reshape(dyShape).mul(labels.toFloat().sub(predictedProbs)), dy.reshape(dyShape).mul(predictedProbs.sub(labels.toFloat()))];\n      };\n\n      return {\n        value: value,\n        gradFunc: gradFunc\n      };\n    });\n    return customOp(labels, logits);\n  };\n\n  __decorate([doc_1.doc({\n    heading: 'Operations',\n    subheading: 'Normalization'\n  }), operation_1.operation], SoftmaxOps, \"softmax\", null);\n\n  __decorate([doc_1.doc({\n    heading: 'Training',\n    subheading: 'Losses',\n    namespace: 'losses'\n  }), operation_1.operation], SoftmaxOps, \"softmaxCrossEntropy\", null);\n\n  return SoftmaxOps;\n}();\n\nexports.SoftmaxOps = SoftmaxOps;","map":null,"metadata":{},"sourceType":"script"}